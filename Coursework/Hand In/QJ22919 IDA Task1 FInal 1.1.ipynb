{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Coursework IDA\n",
    "\n",
    "## Task 1\n",
    "\n",
    "## 1.1. \n",
    "Implement and train a method for automatically classifying texts in the FiQA sentiment analysis\n",
    "dataset as positive, neutral or negative. Refer to the labs, lecture materials and textbook to identify\n",
    "a suitable method. In your report:\n",
    "• Briefly explain how your chosen method works and its main strengths and limitations;\n",
    "• Describe the preprocessing steps and the features you use to represent each text instance;\n",
    "• Explain why you chose those features and preprocessing steps and hypothesise how they\n",
    "will affect your results;\n",
    "• Briefly describe your software implementation.\n",
    "(10 marks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T20:02:35.482647Z",
     "start_time": "2023-06-17T20:02:32.645645900Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use HuggingFace's datasets library to access the financial_phrasebank dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "# pre trained analyser for comparison\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for negation\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T20:02:35.725700400Z",
     "start_time": "2023-06-17T20:02:35.485147100Z"
    }
   },
   "outputs": [],
   "source": [
    "train_files = [\n",
    "    'data_cache/FiQA_ABSA_task1/task1_headline_ABSA_train.json',\n",
    "    'data_cache/FiQA_ABSA_task1/task1_post_ABSA_train.json'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T20:02:45.486911700Z",
     "start_time": "2023-06-17T20:02:43.628584700Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_cache/FiQA_ABSA_task1/task1_headline_ABSA_train.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 46\u001B[0m\n\u001B[0;32m     41\u001B[0m             labels\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(labels)\n\u001B[1;32m---> 46\u001B[0m all_text, all_labels \u001B[38;5;241m=\u001B[39m \u001B[43mload_fiqa_sa_from_json\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_files\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNumber of instances: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(all_text)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNumber of labels: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(all_labels)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[4], line 8\u001B[0m, in \u001B[0;36mload_fiqa_sa_from_json\u001B[1;34m(json_files)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# iterate through each tweet file\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m json_files:\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;66;03m# open file in read mode, with method closes file after getting data stream\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mutf8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handle:\n\u001B[0;32m      9\u001B[0m         \u001B[38;5;66;03m# load file object and convert into json object\u001B[39;00m\n\u001B[0;32m     10\u001B[0m         dataf \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(handle)\n\u001B[0;32m     13\u001B[0m     dataf_text \u001B[38;5;241m=\u001B[39m [dataf[k][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m dataf\u001B[38;5;241m.\u001B[39mkeys()]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    279\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    281\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    282\u001B[0m     )\n\u001B[1;32m--> 284\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data_cache/FiQA_ABSA_task1/task1_headline_ABSA_train.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_fiqa_sa_from_json(json_files):\n",
    "    train_text = []\n",
    "    train_labels = []\n",
    "\n",
    "    # iterate through each tweet file\n",
    "    for file in json_files:\n",
    "        # open file in read mode, with method closes file after getting data stream\n",
    "        with open(file, 'r', encoding = 'utf8') as handle:\n",
    "            # load file object and convert into json object\n",
    "            dataf = json.load(handle)\n",
    "        \n",
    "        \n",
    "        dataf_text = [dataf[k][\"sentence\"] for k in dataf.keys()]\n",
    "        # print(len(dataf_text))\n",
    "        train_text.extend(dataf_text)\n",
    "\n",
    "        dataf_labels = [float(dataf[k][\"info\"][0][\"sentiment_score\"]) for k in dataf.keys()]\n",
    "        # print(len(dataf_labels))\n",
    "        train_labels.extend(dataf_labels)\n",
    "\n",
    "    train_text = np.array(train_text)\n",
    "    train_labels = np.array(train_labels)\n",
    "    \n",
    "    return train_text, train_labels\n",
    "\n",
    "\n",
    "def threshold_scores(scores):\n",
    "    \"\"\"\n",
    "    Convert sentiment scores to discrete labels.\n",
    "    0 = negative.\n",
    "    1 = neutral.\n",
    "    2 = positive.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for score in scores:\n",
    "        if score < -0.25:\n",
    "            labels.append(0)\n",
    "        elif score > 0.32:\n",
    "            labels.append(2)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "            \n",
    "    return np.array(labels)\n",
    "\n",
    "\n",
    "all_text, all_labels = load_fiqa_sa_from_json(train_files)\n",
    "    \n",
    "print(f'Number of instances: {len(all_text)}')\n",
    "print(f'Number of labels: {len(all_labels)}')\n",
    "\n",
    "all_labels = threshold_scores(all_labels)\n",
    "print(f'Number of negative labels: {np.sum(all_labels==0)}')\n",
    "print(f'Number of neutral labels: {np.sum(all_labels==1)}')\n",
    "print(f'Number of positive labels: {np.sum(all_labels==2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(load_fiqa_sa_from_json(train_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(load_fiqa_sa_from_json(train_files)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split test data from training data\n",
    "train_documents, test_documents, train_labels, test_labels = train_test_split(\n",
    "    all_text, \n",
    "    all_labels, \n",
    "    test_size=0.2, \n",
    "    stratify=all_labels  # make sure the same proportion of labels is in the test set and training set\n",
    ")\n",
    "\n",
    "# Split validation data from training data\n",
    "train_documents, val_documents, train_labels, val_labels = train_test_split(\n",
    "    train_documents, \n",
    "    train_labels, \n",
    "    test_size=0.15, \n",
    "    stratify=train_labels  # make sure the same proportion of labels is in the test set and training set\n",
    ")\n",
    "\n",
    "print(f'Number of training instances = {len(train_documents)}')\n",
    "print(f'Number of validation instances = {len(val_documents)}')\n",
    "print(f'Number of test instances = {len(test_documents)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'What does one instance look like from the training set? \\n\\n{train_documents[233]}')\n",
    "print(f'...and here is its corresponding label \\n\\n{train_labels[233]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CountVectorizer can do its own tokenization, but for consistency we want to\n",
    "# carry on using WordNetTokenizer. We write a small wrapper class to enable this:\n",
    "class Tokenizer(object):\n",
    "    def __call__(self, tweets):\n",
    "        return word_tokenize(tweets)\n",
    "\n",
    "    \n",
    "# create stopwords function from nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "vectorizer = CountVectorizer(tokenizer=Tokenizer())  # construct the vectorizer\n",
    "# with stop word removal\n",
    "#vectorizer = CountVectorizer(tokenizer=Tokenizer(), stop_words=stop_words)  # construct the vectorizer\n",
    "\n",
    "vectorizer.fit(train_documents)  # Learn the vocabulary\n",
    "X_train = vectorizer.transform(train_documents)  # extract training set bags of words\n",
    "X_val = vectorizer.transform(val_documents)  # extract test set bags of words\n",
    "X_test = vectorizer.transform(test_documents)  # extract test set bags of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see count vector from training set\n",
    "counts = pd.DataFrame(X_train.toarray(), columns = vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2. Evaluate Method\n",
    "\n",
    "Evaluate your method, then interpret and discuss your results. Include the following points:\n",
    "• Define your performance metrics and state their limitations;\n",
    "• Describe the testing procedure (e.g., how you used each split of the dataset);\n",
    "• Show your results using suitable plots or tables;\n",
    "• How could you improve the method or experimental process? Consider the errors that your\n",
    "method makes.\n",
    "(9 marks)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise and fit classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = classifier.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(val_labels, y_val_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class labels\n",
    "classes = ['0', '1', '2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       xticklabels=classes, yticklabels=classes,\n",
    "       xlabel='Predicted label', ylabel='True label',\n",
    "       title='Confusion matrix')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Add counts to each cell\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc = accuracy_score(val_labels, y_val_pred)\n",
    "print(f'Accuracy = {acc}')\n",
    "\n",
    "prec = precision_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Precision (macro average) = {prec}')\n",
    "\n",
    "rec = recall_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Recall (macro average) = {rec}')\n",
    "\n",
    "f1 = f1_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'F1 score (macro average) = {f1}')\n",
    "\n",
    "# We can get all of these with a per-class breakdown using classification_report:\n",
    "print(classification_report(val_labels, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "### CHANGE THE NAME OF THE CLASSIFIER VARIABLE BELOW TO USE YOUR TRAINED CLASSIFIER\n",
    "feat_likelihoods = np.exp(classifier.feature_log_prob_)  # Use exponential to convert the logs back to probabilities\n",
    "###\n",
    "\n",
    "# WRITE YOUR CODE HERE\n",
    "print(feat_likelihoods[:, vocabulary['a']])\n",
    "print(feat_likelihoods[:, vocabulary['it']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = classifier.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(val_labels, y_val_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define class labels\n",
    "classes = ['0', '1', '2']\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       xticklabels=classes, yticklabels=classes,\n",
    "       xlabel='Predicted label', ylabel='True label',\n",
    "       title='Confusion matrix')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Add counts to each cell\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(val_labels, y_val_pred)\n",
    "print(f'Accuracy = {acc}')\n",
    "\n",
    "prec = precision_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Precision (macro average) = {prec}')\n",
    "\n",
    "rec = recall_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Recall (macro average) = {rec}')\n",
    "\n",
    "f1 = f1_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'F1 score (macro average) = {f1}')\n",
    "\n",
    "# We can get all of these with a per-class breakdown using classification_report:\n",
    "print(classification_report(val_labels, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "acc = accuracy_score(val_labels, y_val_pred)\n",
    "print(f'Accuracy = {acc}')\n",
    "\n",
    "prec = precision_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Precision (macro average) = {prec}')\n",
    "\n",
    "rec = recall_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Recall (macro average) = {rec}')\n",
    "\n",
    "f1 = f1_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'F1 score (macro average) = {f1}')\n",
    "\n",
    "# We can get all of these with a per-class breakdown using classification_report:\n",
    "print(classification_report(val_labels, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_negation(sentence):\n",
    "    # define regex pattern to match words after \"not\", \"n't\", or \"never\"\n",
    "    pattern = r\"(?:(?:(?:not)|(?:n't)|(?:never))\\s+)(\\w+)\"\n",
    "    \n",
    "    # use regex to find and replace words with negation prefix\n",
    "    result = re.sub(pattern, r\" not_\\1\", sentence)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply add_negation to each tweet in the array using a list comprehension\n",
    "all_text_negated = np.array([add_negation(text) for text in all_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert all tweets to lower case\n",
    "all_text_negated = np.char.lower(all_text_negated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split test data from training data\n",
    "train_documents, test_documents, train_labels, test_labels = train_test_split(\n",
    "    all_text_negated, \n",
    "    all_labels, \n",
    "    test_size=0.2, \n",
    "    stratify=all_labels,  # make sure the same proportion of labels is in the test set and training set\n",
    "    random_state= 43\n",
    ")\n",
    "\n",
    "# Split validation data from training data\n",
    "train_documents, val_documents, train_labels, val_labels = train_test_split(\n",
    "    train_documents, \n",
    "    train_labels, \n",
    "    test_size=0.15, \n",
    "    stratify=train_labels,  # make sure the same proportion of labels is in the test set and training set\n",
    "    random_state= 43\n",
    ")\n",
    "\n",
    "print(f'Number of training instances = {len(train_documents)}')\n",
    "print(f'Number of validation instances = {len(val_documents)}')\n",
    "print(f'Number of test instances = {len(test_documents)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'What does one instance look like from the training set? \\n\\n{train_documents[233]}')\n",
    "print(f'...and here is its corresponding label \\n\\n{train_labels[233]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CountVectorizer can do its own tokenization, but for consistency we want to\n",
    "# carry on using WordNetTokenizer. We write a small wrapper class to enable this:\n",
    "class Tokenizer(object):\n",
    "    def __call__(self, tweets):\n",
    "        return word_tokenize(tweets)\n",
    "\n",
    "    \n",
    "# create stopwords function from nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#vectorizer = CountVectorizer(tokenizer=Tokenizer())  # construct the vectorizer\n",
    "\n",
    "# with stop word removal\n",
    "vectorizer = CountVectorizer(tokenizer=Tokenizer(), stop_words=stop_words)  # construct the vectorizer\n",
    "\n",
    "vectorizer.fit(train_documents)  # Learn the vocabulary\n",
    "X_train = vectorizer.transform(train_documents)  # extract training set bags of words\n",
    "X_val = vectorizer.transform(val_documents)  # extract test set bags of words\n",
    "X_test = vectorizer.transform(test_documents)  # extract test set bags of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see count vector from training set\n",
    "counts = pd.DataFrame(X_train.toarray(), columns = vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Naive Bayes Classifier With Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = classifier.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(val_labels, y_val_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class labels\n",
    "classes = ['0', '1', '2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       xticklabels=classes, yticklabels=classes,\n",
    "       xlabel='Predicted label', ylabel='True label',\n",
    "       title='Confusion matrix')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Add counts to each cell\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "acc = accuracy_score(val_labels, y_val_pred)\n",
    "print(f'Accuracy = {acc}')\n",
    "\n",
    "prec = precision_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Precision (macro average) = {prec}')\n",
    "\n",
    "rec = recall_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Recall (macro average) = {rec}')\n",
    "\n",
    "f1 = f1_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'F1 score (macro average) = {f1}')\n",
    "\n",
    "# We can get all of these with a per-class breakdown using classification_report:\n",
    "print(classification_report(val_labels, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "### CHANGE THE NAME OF THE CLASSIFIER VARIABLE BELOW TO USE YOUR TRAINED CLASSIFIER\n",
    "feat_likelihoods = np.exp(classifier.feature_log_prob_)  # Use exponential to convert the logs back to probabilities\n",
    "###\n",
    "\n",
    "# WRITE YOUR CODE HERE\n",
    "#print(feat_likelihoods[:, vocabulary['a']])\n",
    "#print(feat_likelihoods[:, vocabulary['it']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logistic Regression Classifier With Data Processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = classifier.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(val_labels, y_val_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define class labels\n",
    "classes = ['0', '1', '2']\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       xticklabels=classes, yticklabels=classes,\n",
    "       xlabel='Predicted label', ylabel='True label',\n",
    "       title='Confusion matrix')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Add counts to each cell\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "classifier = LogisticRegression( multi_class= 'multinomial')\n",
    "classifier.fit(X_train, train_labels)\n",
    "y_val_pred = classifier.predict(X_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(val_labels, y_val_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Define class labels\n",
    "classes = ['0', '1', '2']\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       xticklabels=classes, yticklabels=classes,\n",
    "       xlabel='Predicted label', ylabel='True label',\n",
    "       title='Confusion matrix')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Add counts to each cell\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Evaluate Method\n",
    "\n",
    "Evaluate your method, then interpret and discuss your results. Include the following points:\n",
    "• Define your performance metrics and state their limitations;\n",
    "• Describe the testing procedure (e.g., how you used each split of the dataset);\n",
    "• Show your results using suitable plots or tables;\n",
    "• How could you improve the method or experimental process? Consider the errors that your\n",
    "method makes.  \n",
    "(9 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "acc = accuracy_score(val_labels, y_val_pred)\n",
    "print(f'Accuracy = {acc}')\n",
    "\n",
    "prec = precision_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Precision (macro average) = {prec}')\n",
    "\n",
    "rec = recall_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Recall (macro average) = {rec}')\n",
    "\n",
    "f1 = f1_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'F1 score (macro average) = {f1}')\n",
    "\n",
    "# We can get all of these with a per-class breakdown using classification_report:\n",
    "print(classification_report(val_labels, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Common Themes & Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. Can you identify common themes or topics associated with negative sentiment or positive\n",
    "sentiment in this dataset?\n",
    "• Explain the method you use to identify themes or topics;\n",
    "• Show your results (e.g., by listing or visualising example topics or themes);\n",
    "• Interpret the results and summarise the limitations of your approach.\n",
    "(12 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_feats_to_show = 10\n",
    "\n",
    "# Flip the index so that values are keys and keys are values:\n",
    "keys = vectorizer.vocabulary_.values()\n",
    "values = vectorizer.vocabulary_.keys()\n",
    "vocab_inverted = dict(zip(keys, values))\n",
    "\n",
    "for c, weights_c in enumerate(classifier.coef_):\n",
    "    print(f'\\nWeights for class {c}:\\n')\n",
    "    strongest_idxs = np.argsort(weights_c)[-n_feats_to_show:]\n",
    "\n",
    "    for idx in strongest_idxs:\n",
    "        print(f'{vocab_inverted[idx]} with weight {weights_c[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_index = all_labels == 2  # compare predictions to gold labels\n",
    "neg_index = all_labels == 0  # compare predictions to gold labels\n",
    "# get the text of tweets where the classifier made an error:\n",
    "pos_tweets = np.array(all_text)[pos_index]\n",
    "neg_tweets = np.array(all_text)[neg_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(pos_tweets)\n",
    "print(pos_tweets[0])\n",
    "print(neg_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_pos = []\n",
    "processed_neg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS # find stopwords\n",
    "\n",
    "np.random.seed(400)  # We fix the random seed to ensure we get consistent results when we repeat the lab.\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in simple_preprocess(text) :  # Tokenize, remove very short and very long words, convert to lower case, remove words containing non-letter characters\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(WordNetLemmatizer().lemmatize(token, 'v'))\n",
    "            \n",
    "    return result\n",
    "\n",
    "# Create lists of preprocessed documents\n",
    "for tweet in pos_tweets:\n",
    "    processed_pos.append(preprocess(tweet))\n",
    "    \n",
    "for tweet in neg_tweets:\n",
    "    processed_neg.append(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_pos[0])\n",
    "print(processed_neg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary_pos = Dictionary(processed_pos) # construct word<->id mappings - it does it in alphabetical order\n",
    "print(dictionary_pos)\n",
    "\n",
    "pos_bow_corpus = [dictionary_pos.doc2bow(tweet) for tweet in processed_pos]\n",
    "\n",
    "dictionary_neg = Dictionary(processed_neg) # construct word<->id mappings - it does it in alphabetical order\n",
    "print(dictionary_neg)\n",
    "\n",
    "neg_bow_corpus = [dictionary_neg.doc2bow(tweet) for tweet in processed_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos_bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neg_bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "lda_pos_model =  LdaModel(pos_bow_corpus, \n",
    "                      num_topics=10, \n",
    "                      id2word=dictionary_pos,                                    \n",
    "                      passes=10,\n",
    "                    ) \n",
    "\n",
    "lda_neg_model =  LdaModel(neg_bow_corpus, \n",
    "                      num_topics=10, \n",
    "                      id2word=dictionary_neg,                                    \n",
    "                      passes=10,\n",
    "                    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_pos_model.print_topics(-1):\n",
    "    print(\"Pos Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "for idx, topic in lda_neg_model.print_topics(-1):\n",
    "    print(\"Neg Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document_idx = 10\n",
    "unseen_document = pos_tweets[test_document_idx]\n",
    "print(unseen_document)\n",
    "\n",
    "#print(f' This document is from newsgroup {newsgroups_test.target_names[newsgroups_test.target[test_document_idx]]}')\n",
    "\n",
    "# Data preprocessing step for the unseen document - It is the same preprocessing we have performed for the training data\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for idx, count in bow_vector:\n",
    "    print(f'{dictionary[idx]}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distribution = lda_model[bow_vector]\n",
    "\n",
    "for index, probability in sorted(topic_distribution, key=lambda tup: -1*tup[1]):\n",
    "    print(\"Index: {}\\nProbability: {}\\t Topic: {}\".format(index, probability, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of tuples ready for model training\n",
    "\n",
    "train_set = list(zip(list_a, list_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Named Entity Recognition (max. 19%)  \n",
    "\n",
    "In scientific research, information extraction can help researchers to discover relevant findings from\n",
    "across a wide body of literature. As a first step, your task is to build a tool for named entity\n",
    "recognition in scientific journal article abstracts. We will be working with the BioNLP 2004 dataset of\n",
    "abstracts from MEDLINE, a database containing journal articles from fields including medicine and\n",
    "pharmacy. The data was collected by searching for the terms ‘human’, ‘blood cells’ and\n",
    "‘transcription factors’, and then annotated with five entity types: DNA, protein, cell type, cell line,\n",
    "RNA. \n",
    "\n",
    "More information can be found in the paper: https://aclanthology.org/W04-1213.pdf .\n",
    "We provide a cache of the data and code for loading the data in ‘data_loader_demo’ in our Github\n",
    "repository, https://github.com/uob-TextAnalytics/intro-labs-public. This script downloaded the data\n",
    "from HuggingFace, where you can also find more information about the dataset:\n",
    "https://huggingface.co/datasets/tner/bionlp2004 .\n",
    "\n",
    "\n",
    "The data is presented in this paper:\n",
    "Nigel Collier, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, and Jin-Dong Kim. 2004. Introduction\n",
    "to the Bio-entity Recognition Task at JNLPBA. In Proceedings of the International Joint Workshop on\n",
    "Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP), pages 73–78,\n",
    "Geneva, Switzerland. COLING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "data_analytics",
   "language": "python",
   "display_name": "data_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
