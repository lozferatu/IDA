{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "55fc4e9a-df49-45e0-98e6-ebd49cf54044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:44.711797600Z",
     "start_time": "2023-07-01T19:44:44.585260700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ec39563-772c-4acb-a00c-4a1dd573162e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BIONLP 2004\n",
    "\n",
    "This dataset contains abstracts from MEDLINE, a database containing journal articles from fields including medicine and pharmacy. \n",
    "The data was collected by searching for the terms ‘human’, ‘blood cells’ and ‘transcription factors’, and then annotated with five entity types: DNA, protein, cell type, cell line, RNA. \n",
    "\n",
    "More information in the paper: https://aclanthology.org/W04-1213.pdf\n",
    "\n",
    "The data can be downloaded from HuggingFace: https://huggingface.co/datasets/tner/bionlp2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bc89a2e6-3882-4021-9e34-dd0f919cc2bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:44.712297700Z",
     "start_time": "2023-07-01T19:44:44.607267300Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b8e09e77-9177-4360-a684-9531dbcf543a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:44.864347700Z",
     "start_time": "2023-07-01T19:44:44.623773300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use HuggingFace's datasets library to access the financial_phrasebank dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d353ede4-863a-4378-976a-4f5607cd7a68",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:47.747487900Z",
     "start_time": "2023-07-01T19:44:44.872349700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset bio_nlp2004 (./data_cache\\tner___bio_nlp2004\\bionlp2004\\1.0.0\\9f41d3f0270b773c2762dee333ae36c29331e2216114a57081f77639fdb5e904)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4942284c1834be99576966e029bc4b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is a dictionary with 3 splits: \n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 16619\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 1927\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 3856\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"tner/bionlp2004\", \n",
    "    cache_dir='./data_cache'\n",
    ")\n",
    "\n",
    "print(f'The dataset is a dictionary with {len(dataset)} splits: \\n\\n{dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b64ae-f9f9-454f-9849-ef77d684dd3e",
   "metadata": {},
   "source": [
    "The dataset is already split into train, validation and test. It may be useful to reformat the DatasetDict object into lists of sentences and tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3e7c8c63-c92e-48c8-8563-44e9f472e158",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:47.984556900Z",
     "start_time": "2023-07-01T19:44:47.738988800Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataset['train'][1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "423b1fe9-4c8b-47c2-b79b-f460e8cc7112",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:52.220291800Z",
     "start_time": "2023-07-01T19:44:47.986057400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sentences_ner = [item['tokens'] for item in dataset['train']]\n",
    "train_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['train']]\n",
    "\n",
    "val_sentences_ner = [item['tokens'] for item in dataset['validation']]\n",
    "val_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['validation']]\n",
    "\n",
    "test_sentences_ner = [item['tokens'] for item in dataset['test']]\n",
    "test_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4da7896a-2dc0-4985-8627-48c77834f599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:52.444857300Z",
     "start_time": "2023-07-01T19:44:52.218791300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences = 16619\n",
      "Number of validation sentences = 1927\n",
      "Number of test sentences = 3856\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training sentences = {len(train_sentences_ner)}')\n",
    "print(f'Number of validation sentences = {len(val_sentences_ner)}')\n",
    "print(f'Number of test sentences = {len(test_sentences_ner)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7d000688-9fbb-4ea3-a95c-becf496ed43f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:52.677923700Z",
     "start_time": "2023-07-01T19:44:52.446857900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does one instance look like from the training set? \n",
      "\n",
      "['Hence', ',', 'PPAR', 'can', 'positively', 'or', 'negatively', 'influence', 'TH', 'action', 'depending', 'on', 'TRE', 'structure', 'and', 'THR', 'isotype', '.']\n",
      "...and here is its corresponding label \n",
      "\n",
      "['0', '0', '3', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '3', '4', '0']\n"
     ]
    }
   ],
   "source": [
    "print(f'What does one instance look like from the training set? \\n\\n{train_sentences_ner[234]}')\n",
    "print(f'...and here is its corresponding label \\n\\n{train_labels_ner[234]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9c293ad5-29d4-4f5f-9213-a37ed2853830",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:53.021523700Z",
     "start_time": "2023-07-01T19:44:52.679925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: ['0' '1' '10' '2' '3' '4' '5' '6' '7' '8' '9']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of unique labels: {np.unique(np.concatenate(train_labels_ner))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3fc291-2394-49f4-99b4-9ba6e32680fe",
   "metadata": {},
   "source": [
    "These are the tags used to annotate the entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "251f5652-afe6-4030-94e6-bf91379b34e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:53.268097600Z",
     "start_time": "2023-07-01T19:44:53.023024500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-DNA', 2: 'I-DNA', 3: 'B-protein', 4: 'I-protein', 5: 'B-cell_type', 6: 'I-cell_type', 7: 'B-cell_line', 8: 'I-cell_line', 9: 'B-RNA', 10: 'I-RNA'}\n"
     ]
    }
   ],
   "source": [
    "# mapping from labels to the tags\n",
    "\n",
    "id2label = {\n",
    "    \"O\": 0,\n",
    "    \"B-DNA\": 1,\n",
    "    \"I-DNA\": 2,\n",
    "    \"B-protein\": 3,\n",
    "    \"I-protein\": 4,\n",
    "    \"B-cell_type\": 5,\n",
    "    \"I-cell_type\": 6,\n",
    "    \"B-cell_line\": 7,\n",
    "    \"I-cell_line\": 8,\n",
    "    \"B-RNA\": 9,\n",
    "    \"I-RNA\": 10\n",
    "}\n",
    "\n",
    "label2id = {v:k for k, v in id2label.items()}\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '7', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '3', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "print(train_labels_ner[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:53.501666200Z",
     "start_time": "2023-07-01T19:44:53.270598400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e60b82e0-c93d-4bf3-af10-5db0e0423c5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:53.811766700Z",
     "start_time": "2023-07-01T19:44:53.503166800Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# convert label ref from string to integer label\n",
    "int_train_labels = []\n",
    "for sent in train_labels_ner:\n",
    "    sent = list(map(int, sent)) \n",
    "    int_train_labels.append(sent)\n",
    "    # print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0354cac2-1496-4571-bb6e-91518009f7e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:54.339424600Z",
     "start_time": "2023-07-01T19:44:53.813768200Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# working int to label\n",
    "train_lab_full = []\n",
    "\n",
    "# convert list of label indexes to label string value from label3id lookup\n",
    "for vector in int_train_labels:\n",
    "    #vector = list(map(int, vector)) \n",
    "    #print(sent)\n",
    "    sent_labels = []\n",
    "    for label in vector:\n",
    "        converted  = label2id[label]\n",
    "        sent_labels.append(converted)\n",
    "        #print(converted)\n",
    "        #res = list(zip(train_sentences_ner, train_labels_ner[idx]))\n",
    "        #train_set.append(res)\n",
    "    train_lab_full.append(list(sent_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3e7aa3b6-fdb7-4217-a8d2-3ac82de3a56f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:54.564989900Z",
     "start_time": "2023-07-01T19:44:54.340925700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "['O', 'B-protein', 'O', 'O', 'O', 'O', 'O', 'O', 'B-protein', 'I-protein', 'O', 'O', 'O', 'O', 'B-protein', 'I-protein', 'O', 'O', 'O', 'O', 'B-protein', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "36\n",
      "['0', '3', '0', '0', '0', '0', '0', '0', '3', '4', '0', '0', '0', '0', '3', '4', '0', '0', '0', '0', '3', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "# Check lists correspond\n",
    "print(len(train_lab_full[1]))\n",
    "print(train_lab_full[1])\n",
    "print(len(train_labels_ner[1]))\n",
    "print(train_labels_ner[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "#train_sentences_ner[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:54.803563100Z",
     "start_time": "2023-07-01T19:44:54.572993100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ef61c99b-01c8-4b54-aaa0-d307edc75c20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:55.098156100Z",
     "start_time": "2023-07-01T19:44:54.806064700Z"
    }
   },
   "outputs": [],
   "source": [
    "# working zip\n",
    "train_set = [list(zip(sentence, train_lab_full[idx])) for idx, sentence in enumerate(train_sentences_ner)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "#train_set[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:55.330584700Z",
     "start_time": "2023-07-01T19:44:55.100157Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "# convert test labels from string to integer label\n",
    "int_val_labels = []\n",
    "for sent in val_labels_ner:\n",
    "    sent = list(map(int, sent))\n",
    "    int_val_labels.append(sent)\n",
    "    #print(sent)\n",
    "# working int to label\n",
    "val_lab_full = []\n",
    "\n",
    "for vector in int_val_labels:\n",
    "    #vector = list(map(int, vector))\n",
    "    #print(sent)\n",
    "    sent_labels = []\n",
    "    for label in vector:\n",
    "        converted = label2id[label]\n",
    "        sent_labels.append(converted)\n",
    "        #print(converted)\n",
    "        #res = list(zip(train_sentences_ner, train_labels_ner[idx]))\n",
    "        #train_set.append(res)\n",
    "    val_lab_full.append(list(sent_labels))\n",
    "\n",
    "val_set = [list(zip(sentence, val_lab_full[idx])) for idx, sentence in enumerate(val_sentences_ner)]\n",
    "# example\n",
    "#val_set[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:55.594159700Z",
     "start_time": "2023-07-01T19:44:55.332084800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:55.608162400Z",
     "start_time": "2023-07-01T19:44:55.596160500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6e007294-cab0-4117-990f-0d3dea1b33fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:55.888255300Z",
     "start_time": "2023-07-01T19:44:55.610164600Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert test labels from string to integer label\n",
    "int_test_labels = []\n",
    "for sent in test_labels_ner:\n",
    "    sent = list(map(int, sent)) \n",
    "    int_test_labels.append(sent)\n",
    "    #print(sent)\n",
    "\n",
    "# working int to label\n",
    "test_lab_full = []\n",
    "\n",
    "for vector in int_test_labels:\n",
    "    #vector = list(map(int, vector))\n",
    "    #print(sent)\n",
    "    sent_labels = []\n",
    "    for label in vector:\n",
    "        converted  = label2id[label]\n",
    "        sent_labels.append(converted)\n",
    "        #print(converted)\n",
    "        #res = list(zip(train_sentences_ner, train_labels_ner[idx]))\n",
    "        #train_set.append(res)\n",
    "    test_lab_full.append(list(sent_labels))\n",
    "\n",
    "test_set = [list(zip(sentence, test_lab_full[idx])) for idx, sentence in enumerate(test_sentences_ner)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "93874e80-fde6-4c08-a214-5b7389656d7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:56.121821400Z",
     "start_time": "2023-07-01T19:44:55.890755300Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_sentences_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c0eda321-8c5f-4c52-8929-fe1eee365c22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:44:56.135830600Z",
     "start_time": "2023-07-01T19:44:56.122821600Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f7215037-4a74-4312-baac-dbea5f114da1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:46:22.249257300Z",
     "start_time": "2023-07-01T19:44:56.137831600Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Train a CRF NER tagger\n",
    "def train_CRF_NER_tagger(train_set):\n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    tagger = nltk.tag.CRFTagger()\n",
    "    tagger.train(train_set, 'model.crf.tagger')\n",
    "    return tagger  # return the trained model\n",
    "\n",
    "tagger = train_CRF_NER_tagger(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "269e4ce0-fd92-410a-8722-c04a1a5bb5d3",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-01T19:46:22.493011400Z",
     "start_time": "2023-07-01T19:46:22.252257500Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_spans(tagged_sents):\n",
    "    \"\"\"\n",
    "    Extract a list of tagged spans for each named entity type, \n",
    "    where each span is represented by a tuple containing the \n",
    "    start token and end token indexes.\n",
    "    \n",
    "    returns: a dictionary containing a list of spans for each entity type.\n",
    "    \"\"\"\n",
    "    spans = {}\n",
    "        \n",
    "    for sidx, sent in enumerate(tagged_sents):\n",
    "        start = -1\n",
    "        entity_type = None\n",
    "        for i, (tok, lab) in enumerate(sent):\n",
    "            if 'B-' in lab:\n",
    "                start = i\n",
    "                end = i + 1\n",
    "                entity_type = lab[2:]\n",
    "            elif 'I-' in lab:\n",
    "                end = i + 1\n",
    "            elif lab == 'O' and start >= 0:\n",
    "                \n",
    "                if entity_type not in spans:\n",
    "                    spans[entity_type] = []\n",
    "                \n",
    "                spans[entity_type].append((start, end, sidx))\n",
    "                start = -1      \n",
    "        # Sometimes an I-token is the last token in the sentence, so we still have to add the span to the list\n",
    "        if start >= 0:    \n",
    "            if entity_type not in spans:\n",
    "                spans[entity_type] = []\n",
    "                \n",
    "            spans[entity_type].append((start, end, sidx))\n",
    "                \n",
    "    return spans\n",
    "\n",
    "\n",
    "def cal_span_level_f1(test_sents, test_sents_with_pred):\n",
    "    # get a list of spans from the test set labels\n",
    "    gold_spans = extract_spans(test_sents)\n",
    "\n",
    "    # get a list of spans predicted by our tagger\n",
    "    pred_spans = extract_spans(test_sents_with_pred)\n",
    "    \n",
    "    # compute the metrics for each class:\n",
    "    f1_per_class = []\n",
    "    \n",
    "    ne_types = gold_spans.keys()  # get the list of named entity types (not the tags)\n",
    "    \n",
    "    for ne_type in ne_types:\n",
    "        # compute the confusion matrix\n",
    "        true_pos = 0\n",
    "        false_pos = 0\n",
    "        \n",
    "        for span in pred_spans[ne_type]:\n",
    "            if span in gold_spans[ne_type]:\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                false_pos += 1\n",
    "                \n",
    "        false_neg = 0\n",
    "        for span in gold_spans[ne_type]:\n",
    "            if span not in pred_spans[ne_type]:\n",
    "                false_neg += 1\n",
    "                \n",
    "        if true_pos + false_pos == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = true_pos / float(true_pos + false_pos)\n",
    "            \n",
    "        if true_pos + false_neg == 0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = true_pos / float(true_pos + false_neg)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            \n",
    "        f1_per_class.append(f1)\n",
    "        print(f'F1 score for class {ne_type} = {f1}')\n",
    "        \n",
    "    print(f'Macro-average f1 score = {np.mean(f1_per_class)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "predicted_tags = tagger.tag_sents(test_sentences_ner)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T19:46:23.226226900Z",
     "start_time": "2023-07-01T19:46:22.496512900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for class protein = 0.6533282031550597\n",
      "F1 score for class cell_type = 0.6299212598425197\n",
      "F1 score for class DNA = 0.5850476668339187\n",
      "F1 score for class cell_line = 0.4785788923719958\n",
      "F1 score for class RNA = 0.6017699115044248\n",
      "Macro-average f1 score = 0.5897291867415838\n"
     ]
    }
   ],
   "source": [
    "cal_span_level_f1(test_set, predicted_tags)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T19:46:24.214514Z",
     "start_time": "2023-07-01T19:46:23.224227100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "63552679-767f-46d7-9792-37d02f501734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T19:46:24.509521300Z",
     "start_time": "2023-07-01T19:46:24.216514200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cell_line', 'protein', 'DNA', 'cell_type', 'RNA'])\n"
     ]
    }
   ],
   "source": [
    "span_dict = print(extract_spans(train_set).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Since', 'O'), ('HUVECs', 'B-cell_line'), ('released', 'O'), ('superoxide', 'O'), ('anions', 'O'), ('in', 'O'), ('response', 'O'), ('to', 'O'), ('TNF', 'O'), (',', 'O'), ('and', 'O'), ('H2O2', 'O'), ('induces', 'O'), ('VCAM-1', 'B-protein'), (',', 'O'), ('PDTC', 'O'), ('may', 'O'), ('act', 'O'), ('as', 'O'), ('a', 'O'), ('radical', 'O'), ('scavenger', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T19:46:24.742594200Z",
     "start_time": "2023-07-01T19:46:24.511022700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[114], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mspan_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprotein\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;241m0\u001B[39m])\n",
      "\u001B[1;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(span_dict['protein'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T19:46:24.984661Z",
     "start_time": "2023-07-01T19:46:24.743594700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if span_dict and 'DNA' in span_dict:\n",
    "    print(span_dict['protein'])\n",
    "else:\n",
    "    print(\"span_dict is None or 'protein' is not in the dictionary.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted_tags ="
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lexical and n-grams captured in POS, will derive from lemma and POS_TAGS\n",
    "\n",
    "## CRF Additional Features\n",
    "# Current word\n",
    "# previous word\n",
    "# next word\n",
    "# (orthographics =or) is it capitalised?\n",
    "# (orthographics =or) Does it have punctuation?\n",
    "# (orthographics =or) Does it have a number?\n",
    "# (affix information = af) Suffixes & Preffixs up to length 3\n",
    "# (worshape = sh) wordshape\n",
    "\n",
    "import re, unicodedata\n",
    "\n",
    "# custom tagger that inherits from nltk.tag.CRFtagger\n",
    "#\n",
    "class CustomCRFTagger(nltk.tag.CRFTagger):\n",
    "    _current_tokens = None\n",
    "\n",
    "    def _get_features(self, tokens, idx):\n",
    "\n",
    "            token = tokens[idx]\n",
    "\n",
    "            feature_list = []\n",
    "\n",
    "            if not token:\n",
    "                return feature_list\n",
    "\n",
    "            # Current word\n",
    "            feature_list.append(\"WORD_\" + token)\n",
    "\n",
    "            # Previous / Next Word\n",
    "            if idx > 0:\n",
    "                feature_list.append(\"PREVWORD_\" + tokens[idx-1]) # if first word there is no previous, so skip first word\n",
    "            if idx < len(tokens)-1:\n",
    "                feature_list.append(\"NEXTWORD_\" + tokens[idx+1]) # if last word there is no next, so don't go to last word\n",
    "\n",
    "            # Capitalisation\n",
    "            if token[0].isupper():\n",
    "                feature_list.append(\"CAPITALIZATION\")\n",
    "\n",
    "            # Number\n",
    "            if re.search(self._pattern, token) is not None:\n",
    "                feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "            # Punctuation\n",
    "            punc_cat = {\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"}\n",
    "            if all(unicodedata.category(x) in punc_cat for x in token):\n",
    "                feature_list.append(\"PUNCTUATION\")\n",
    "\n",
    "            # new feature 'SUF_fix'\n",
    "            if len(token) > 1:\n",
    "                feature_list.append(\"SUF_\" + token[-1:])\n",
    "            if len(token) > 2:\n",
    "                feature_list.append(\"SUF_\" + token[-2:])\n",
    "            if len(token) > 3:\n",
    "                feature_list.append(\"SUF_\" + token[-3:])\n",
    "\n",
    "            # new feature 'PRE_fix'\n",
    "            if len(token) > 1:\n",
    "                feature_list.append(\"PRE_\" + token[:1])\n",
    "            if len(token) > 2:\n",
    "                feature_list.append(\"PRE_\" + token[:2])\n",
    "            if len(token) > 3:\n",
    "                feature_list.append(\"PRE_\" + token[:3])\n",
    "\n",
    "            # Wordshape\n",
    "            wordshape = \"\"\n",
    "            for char in token:\n",
    "                if char.isupper():\n",
    "                    wordshape += \"U\"\n",
    "                elif char.islower():\n",
    "                    wordshape += \"L\"\n",
    "                elif char.isdigit():\n",
    "                    wordshape += \"D\"\n",
    "                else:\n",
    "                    wordshape += \"P\"\n",
    "            feature_list.append(\"WORDSHAPE_\" + wordshape)\n",
    "\n",
    "            return feature_list\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
