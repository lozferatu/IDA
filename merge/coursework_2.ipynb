{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Coursework IDA\n",
    "\n",
    "## Task 1\n",
    "\n",
    "## 1.1. \n",
    "Implement and train a method for automatically classifying texts in the FiQA sentiment analysis\n",
    "dataset as positive, neutral or negative. Refer to the labs, lecture materials and textbook to identify\n",
    "a suitable method. In your report:\n",
    "• Briefly explain how your chosen method works and its main strengths and limitations;\n",
    "• Describe the preprocessing steps and the features you use to represent each text instance;\n",
    "• Explain why you chose those features and preprocessing steps and hypothesise how they\n",
    "will affect your results;\n",
    "• Briefly describe your software implementation.\n",
    "(10 marks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:56:58.856007500Z",
     "start_time": "2023-06-16T14:56:57.675293200Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use HuggingFace's datasets library to access the financial_phrasebank dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:56:59.011029600Z",
     "start_time": "2023-06-16T14:56:58.858007800Z"
    }
   },
   "outputs": [],
   "source": [
    "train_files = [\n",
    "    # 'data_cache/FiQA_ABSA_task1/task1_headline_ABSA_train.json',\n",
    "    'data_cache/FiQA_ABSA_task1/task1_post_ABSA_train.json'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:56:59.180522800Z",
     "start_time": "2023-06-16T14:56:59.015030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 675\n",
      "Number of labels: 675\n",
      "Number of negative labels: 203\n",
      "Number of neutral labels: 74\n",
      "Number of positive labels: 398\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_fiqa_sa_from_json(json_files):\n",
    "    train_text = []\n",
    "    train_labels = []\n",
    "\n",
    "    # iterate through each tweet file\n",
    "    for file in json_files:\n",
    "        # open file in read mode, with method closes file after getting data stream\n",
    "        with open(file, 'r', encoding = 'utf8') as handle:\n",
    "            # load file object and convert into json object\n",
    "            dataf = json.load(handle)\n",
    "        \n",
    "        \n",
    "        dataf_text = [dataf[k][\"sentence\"] for k in dataf.keys()]\n",
    "        # print(len(dataf_text))\n",
    "        train_text.extend(dataf_text)\n",
    "\n",
    "        dataf_labels = [float(dataf[k][\"info\"][0][\"sentiment_score\"]) for k in dataf.keys()]\n",
    "        # print(len(dataf_labels))\n",
    "        train_labels.extend(dataf_labels)\n",
    "\n",
    "    train_text = np.array(train_text)\n",
    "    train_labels = np.array(train_labels)\n",
    "    \n",
    "    return train_text, train_labels\n",
    "\n",
    "\n",
    "def threshold_scores(scores):\n",
    "    \"\"\"\n",
    "    Convert sentiment scores to discrete labels.\n",
    "    0 = negative.\n",
    "    1 = neutral.\n",
    "    2 = positive.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for score in scores:\n",
    "        if score < -0.2:\n",
    "            labels.append(0)\n",
    "        elif score > 0.2:\n",
    "            labels.append(2)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "            \n",
    "    return np.array(labels)\n",
    "\n",
    "\n",
    "all_text, all_labels = load_fiqa_sa_from_json(train_files)\n",
    "    \n",
    "print(f'Number of instances: {len(all_text)}')\n",
    "print(f'Number of labels: {len(all_labels)}')\n",
    "\n",
    "all_labels = threshold_scores(all_labels)\n",
    "print(f'Number of negative labels: {np.sum(all_labels==0)}')\n",
    "print(f'Number of neutral labels: {np.sum(all_labels==1)}')\n",
    "print(f'Number of positive labels: {np.sum(all_labels==2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:56:59.351555Z",
     "start_time": "2023-06-16T14:56:59.181023400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tuple"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(load_fiqa_sa_from_json(train_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:56:59.522084700Z",
     "start_time": "2023-06-16T14:56:59.354054700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675\n"
     ]
    }
   ],
   "source": [
    "print(len(load_fiqa_sa_from_json(train_files)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:00.451266700Z",
     "start_time": "2023-06-16T14:56:59.523584900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances = 459\n",
      "Number of validation instances = 81\n",
      "Number of test instances = 135\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split test data from training data\n",
    "train_documents, test_documents, train_labels, test_labels = train_test_split(\n",
    "    all_text, \n",
    "    all_labels, \n",
    "    test_size=0.2, \n",
    "    stratify=all_labels,  # make sure the same proportion of labels is in the test set and training set\n",
    "    random_state = 43\n",
    ")\n",
    "\n",
    "# Split validation data from training data\n",
    "train_documents, val_documents, train_labels, val_labels = train_test_split(\n",
    "    train_documents, \n",
    "    train_labels, \n",
    "    test_size=0.15, \n",
    "    stratify=train_labels,  # make sure the same proportion of labels is in the test set and training set\n",
    "    random_state = 43\n",
    ")\n",
    "\n",
    "print(f'Number of training instances = {len(train_documents)}')\n",
    "print(f'Number of validation instances = {len(val_documents)}')\n",
    "print(f'Number of test instances = {len(test_documents)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:00.652803300Z",
     "start_time": "2023-06-16T14:57:00.454268300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does one instance look like from the training set? \n",
      "\n",
      "Facebook, near a buy point last week, faces a different technical test today https://t.co/c72LLMpiNM $FB $AAPL $NFLX https://t.co/fPFbYTYPuY\n",
      "...and here is its corresponding label \n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(f'What does one instance look like from the training set? \\n\\n{train_documents[233]}')\n",
    "print(f'...and here is its corresponding label \\n\\n{train_labels[233]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:01.486945800Z",
     "start_time": "2023-06-16T14:57:00.655302700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loz\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# CountVectorizer can do its own tokenization, but for consistency we want to\n",
    "# carry on using WordNetTokenizer. We write a small wrapper class to enable this:\n",
    "class Tokenizer(object):\n",
    "    def __call__(self, tweets):\n",
    "        return word_tokenize(tweets)\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=Tokenizer())  # construct the vectorizer\n",
    "\n",
    "vectorizer.fit(train_documents)  # Learn the vocabulary\n",
    "X_train = vectorizer.transform(train_documents)  # extract training set bags of words\n",
    "X_val = vectorizer.transform(val_documents)  # extract test set bags of words\n",
    "X_test = vectorizer.transform(test_documents)  # extract test set bags of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:01.497447600Z",
     "start_time": "2023-06-16T14:57:01.460942Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:01.755993600Z",
     "start_time": "2023-06-16T14:57:01.475945900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "MultinomialNB()"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:02.002536900Z",
     "start_time": "2023-06-16T14:57:01.739492Z"
    }
   },
   "outputs": [],
   "source": [
    "y_val_pred = classifier.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:02.281586900Z",
     "start_time": "2023-06-16T14:57:02.005037200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.654320987654321\n",
      "Precision (macro average) = 0.4567099567099568\n",
      "Recall (macro average) = 0.4236111111111111\n",
      "F1 score (macro average) = 0.40661824051654566\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.33      0.46        24\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.64      0.94      0.76        48\n",
      "\n",
      "    accuracy                           0.65        81\n",
      "   macro avg       0.46      0.42      0.41        81\n",
      "weighted avg       0.60      0.65      0.59        81\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loz\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\loz\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\loz\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\loz\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "acc = accuracy_score(val_labels, y_val_pred)\n",
    "print(f'Accuracy = {acc}')\n",
    "\n",
    "prec = precision_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Precision (macro average) = {prec}')\n",
    "\n",
    "rec = recall_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Recall (macro average) = {rec}')\n",
    "\n",
    "f1 = f1_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'F1 score (macro average) = {f1}')\n",
    "\n",
    "# We can get all of these with a per-class breakdown using classification_report:\n",
    "print(classification_report(val_labels, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:02.528129900Z",
     "start_time": "2023-06-16T14:57:02.282585900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00563792 0.00534283 0.01019968]\n",
      "[0.00480267 0.00356189 0.00330412]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "### CHANGE THE NAME OF THE CLASSIFIER VARIABLE BELOW TO USE YOUR TRAINED CLASSIFIER\n",
    "feat_likelihoods = np.exp(classifier.feature_log_prob_)  # Use exponential to convert the logs back to probabilities\n",
    "###\n",
    "\n",
    "# WRITE YOUR CODE HERE\n",
    "print(feat_likelihoods[:, vocabulary['a']])\n",
    "print(feat_likelihoods[:, vocabulary['it']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:02.544633100Z",
     "start_time": "2023-06-16T14:57:02.532630500Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logistic Regression Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:02.870188500Z",
     "start_time": "2023-06-16T14:57:02.545633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression()"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:02.906194700Z",
     "start_time": "2023-06-16T14:57:02.871188700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:03.155239100Z",
     "start_time": "2023-06-16T14:57:02.887691400Z"
    }
   },
   "outputs": [],
   "source": [
    "y_val_pred = classifier.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:03.172742200Z",
     "start_time": "2023-06-16T14:57:03.150238700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Evaluate Method\n",
    "\n",
    "Evaluate your method, then interpret and discuss your results. Include the following points:\n",
    "• Define your performance metrics and state their limitations;\n",
    "• Describe the testing procedure (e.g., how you used each split of the dataset);\n",
    "• Show your results using suitable plots or tables;\n",
    "• How could you improve the method or experimental process? Consider the errors that your\n",
    "method makes.  \n",
    "(9 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:03.199747800Z",
     "start_time": "2023-06-16T14:57:03.165740800Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:03.460293200Z",
     "start_time": "2023-06-16T14:57:03.182744200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6790123456790124\n",
      "Precision (macro average) = 0.7813620071684587\n",
      "Recall (macro average) = 0.4953703703703704\n",
      "F1 score (macro average) = 0.5116883116883116\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.50      0.57        24\n",
      "           1       1.00      0.11      0.20         9\n",
      "           2       0.68      0.88      0.76        48\n",
      "\n",
      "    accuracy                           0.68        81\n",
      "   macro avg       0.78      0.50      0.51        81\n",
      "weighted avg       0.71      0.68      0.64        81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "acc = accuracy_score(val_labels, y_val_pred)\n",
    "print(f'Accuracy = {acc}')\n",
    "\n",
    "prec = precision_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Precision (macro average) = {prec}')\n",
    "\n",
    "rec = recall_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Recall (macro average) = {rec}')\n",
    "\n",
    "f1 = f1_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'F1 score (macro average) = {f1}')\n",
    "\n",
    "# We can get all of these with a per-class breakdown using classification_report:\n",
    "print(classification_report(val_labels, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:03.474294800Z",
     "start_time": "2023-06-16T14:57:03.461293700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:03.516302Z",
     "start_time": "2023-06-16T14:57:03.475795200Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:03.522803100Z",
     "start_time": "2023-06-16T14:57:03.491798100Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:03.522803100Z",
     "start_time": "2023-06-16T14:57:03.516302Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Common Themes & Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. Can you identify common themes or topics associated with negative sentiment or positive\n",
    "sentiment in this dataset?\n",
    "• Explain the method you use to identify themes or topics;\n",
    "• Show your results (e.g., by listing or visualising example topics or themes);\n",
    "• Interpret the results and summarise the limitations of your approach.\n",
    "(12 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:03.800352800Z",
     "start_time": "2023-06-16T14:57:03.522803100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights for class 0:\n",
      "\n",
      "downgraded with weight 0.5185444501971485\n",
      "might with weight 0.5521056340933332\n",
      "lower with weight 0.5545015307535974\n",
      "tsla with weight 0.5561943080907948\n",
      "model with weight 0.5848685720546261\n",
      "downside with weight 0.6024350157845776\n",
      "sbux with weight 0.6975188496435217\n",
      "spy with weight 0.7108861954239818\n",
      "down with weight 1.0078950248461631\n",
      "short with weight 1.0895618849349944\n",
      "\n",
      "Weights for class 1:\n",
      "\n",
      "but with weight 0.5098350676115879\n",
      "not with weight 0.5126206823906297\n",
      "under with weight 0.5203803357645186\n",
      "here with weight 0.5223132194115013\n",
      "today with weight 0.5589399753330635\n",
      "sells with weight 0.5635830752801967\n",
      "aapl with weight 0.6562792693583221\n",
      "rt with weight 0.6977507971335787\n",
      "nvda with weight 0.7028193840768459\n",
      "sideways with weight 0.7028193840768459\n",
      "\n",
      "Weights for class 2:\n",
      "\n",
      "upgrades with weight 0.43920196388803384\n",
      "positive with weight 0.4531631501095449\n",
      "stocks with weight 0.4826728950064811\n",
      "good with weight 0.510436117458946\n",
      "calls with weight 0.5506516322243667\n",
      "bullish with weight 0.6394260117224729\n",
      "up with weight 0.6916908008674266\n",
      "buy with weight 0.7020331775886556\n",
      "higher with weight 0.7809348469291953\n",
      "long with weight 1.2596815099420275\n"
     ]
    }
   ],
   "source": [
    "n_feats_to_show = 10\n",
    "\n",
    "# Flip the index so that values are keys and keys are values:\n",
    "keys = vectorizer.vocabulary_.values()\n",
    "values = vectorizer.vocabulary_.keys()\n",
    "vocab_inverted = dict(zip(keys, values))\n",
    "\n",
    "for c, weights_c in enumerate(classifier.coef_):\n",
    "    print(f'\\nWeights for class {c}:\\n')\n",
    "    strongest_idxs = np.argsort(weights_c)[-n_feats_to_show:]\n",
    "\n",
    "    for idx in strongest_idxs:\n",
    "        print(f'{vocab_inverted[idx]} with weight {weights_c[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:03.834358400Z",
     "start_time": "2023-06-16T14:57:03.801852200Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:03.844860900Z",
     "start_time": "2023-06-16T14:57:03.815855400Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:04.098405300Z",
     "start_time": "2023-06-16T14:57:03.834358400Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_index = all_labels == 2  # compare predictions to gold labels\n",
    "neg_index = all_labels == 0  # compare predictions to gold labels\n",
    "# get the text of tweets where the classifier made an error:\n",
    "pos_tweets = np.array(all_text)[pos_index]\n",
    "neg_tweets = np.array(all_text)[neg_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:04.365450800Z",
     "start_time": "2023-06-16T14:57:04.096405200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slowly adding some $FIO here but gotta be careful. This will be one of biggest winners in 2012\n",
      "I am not optimistic about $amzn both fundementals and charts look like poopoo this quarter.\n"
     ]
    }
   ],
   "source": [
    "#type(pos_tweets)\n",
    "print(pos_tweets[0])\n",
    "print(neg_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:04.607498400Z",
     "start_time": "2023-06-16T14:57:04.360450900Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_pos = []\n",
    "processed_neg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:06.250792100Z",
     "start_time": "2023-06-16T14:57:04.608997900Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS # find stopwords\n",
    "\n",
    "np.random.seed(400)  # We fix the random seed to ensure we get consistent results when we repeat the lab.\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in simple_preprocess(text) :  # Tokenize, remove very short and very long words, convert to lower case, remove words containing non-letter characters\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(WordNetLemmatizer().lemmatize(token, 'v'))\n",
    "            \n",
    "    return result\n",
    "\n",
    "# Create lists of preprocessed documents\n",
    "for tweet in pos_tweets:\n",
    "    processed_pos.append(preprocess(tweet))\n",
    "    \n",
    "for tweet in neg_tweets:\n",
    "    processed_neg.append(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:06.482831600Z",
     "start_time": "2023-06-16T14:57:06.251292400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slowly', 'add', 'fio', 'gotta', 'careful', 'biggest', 'winners']\n",
      "['optimistic', 'amzn', 'fundementals', 'chart', 'look', 'like', 'poopoo', 'quarter']\n"
     ]
    }
   ],
   "source": [
    "print(processed_pos[0])\n",
    "print(processed_neg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:06.497334500Z",
     "start_time": "2023-06-16T14:57:06.483832700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:06.746379300Z",
     "start_time": "2023-06-16T14:57:06.498834500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1514 unique tokens: ['add', 'biggest', 'careful', 'fio', 'gotta']...)\n",
      "Dictionary(887 unique tokens: ['amzn', 'chart', 'fundementals', 'like', 'look']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary_pos = Dictionary(processed_pos) # construct word<->id mappings - it does it in alphabetical order\n",
    "print(dictionary_pos)\n",
    "\n",
    "pos_bow_corpus = [dictionary_pos.doc2bow(tweet) for tweet in processed_pos]\n",
    "\n",
    "dictionary_neg = Dictionary(processed_neg) # construct word<->id mappings - it does it in alphabetical order\n",
    "print(dictionary_neg)\n",
    "\n",
    "neg_bow_corpus = [dictionary_neg.doc2bow(tweet) for tweet in processed_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:06.978419800Z",
     "start_time": "2023-06-16T14:57:06.747379500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "398"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:07.210960Z",
     "start_time": "2023-06-16T14:57:06.979420100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "203"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:09.274737400Z",
     "start_time": "2023-06-16T14:57:07.212460700Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "lda_pos_model =  LdaModel(pos_bow_corpus, \n",
    "                      num_topics=10, \n",
    "                      id2word=dictionary_pos,                                    \n",
    "                      passes=10,\n",
    "                    ) \n",
    "\n",
    "lda_neg_model =  LdaModel(neg_bow_corpus, \n",
    "                      num_topics=10, \n",
    "                      id2word=dictionary_neg,                                    \n",
    "                      passes=10,\n",
    "                    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:09.520781100Z",
     "start_time": "2023-06-16T14:57:09.275237600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos Topic: 0 \n",
      "Words: 0.026*\"http\" + 0.026*\"stks\" + 0.015*\"strong\" + 0.012*\"today\" + 0.011*\"look\" + 0.009*\"https\" + 0.009*\"aapl\" + 0.006*\"resistance\" + 0.006*\"year\" + 0.006*\"outperform\"\n",
      "\n",
      "\n",
      "Pos Topic: 1 \n",
      "Words: 0.027*\"http\" + 0.026*\"stks\" + 0.020*\"https\" + 0.019*\"long\" + 0.015*\"aapl\" + 0.010*\"close\" + 0.010*\"break\" + 0.009*\"stock\" + 0.009*\"day\" + 0.009*\"spy\"\n",
      "\n",
      "\n",
      "Pos Topic: 2 \n",
      "Words: 0.022*\"stks\" + 0.022*\"http\" + 0.015*\"nice\" + 0.015*\"bounce\" + 0.013*\"today\" + 0.013*\"aapl\" + 0.011*\"earn\" + 0.011*\"buy\" + 0.009*\"fb\" + 0.008*\"long\"\n",
      "\n",
      "\n",
      "Pos Topic: 3 \n",
      "Words: 0.020*\"higher\" + 0.016*\"stks\" + 0.016*\"http\" + 0.015*\"aapl\" + 0.013*\"https\" + 0.011*\"low\" + 0.011*\"buy\" + 0.011*\"high\" + 0.009*\"breakout\" + 0.009*\"tomorrow\"\n",
      "\n",
      "\n",
      "Pos Topic: 4 \n",
      "Words: 0.013*\"buy\" + 0.013*\"long\" + 0.013*\"bbry\" + 0.010*\"today\" + 0.008*\"call\" + 0.008*\"pop\" + 0.008*\"report\" + 0.008*\"yhoo\" + 0.008*\"eps\" + 0.005*\"stock\"\n",
      "\n",
      "\n",
      "Pos Topic: 5 \n",
      "Words: 0.024*\"stks\" + 0.024*\"http\" + 0.014*\"long\" + 0.011*\"day\" + 0.009*\"aapl\" + 0.008*\"good\" + 0.008*\"keep\" + 0.008*\"https\" + 0.006*\"bullish\" + 0.006*\"green\"\n",
      "\n",
      "\n",
      "Pos Topic: 6 \n",
      "Words: 0.031*\"buy\" + 0.017*\"stks\" + 0.017*\"http\" + 0.016*\"https\" + 0.014*\"upgrade\" + 0.013*\"tsla\" + 0.013*\"look\" + 0.012*\"signal\" + 0.012*\"amzn\" + 0.009*\"time\"\n",
      "\n",
      "\n",
      "Pos Topic: 7 \n",
      "Words: 0.017*\"https\" + 0.013*\"http\" + 0.012*\"stks\" + 0.010*\"call\" + 0.010*\"long\" + 0.010*\"fb\" + 0.010*\"break\" + 0.008*\"look\" + 0.008*\"chart\" + 0.008*\"run\"\n",
      "\n",
      "\n",
      "Pos Topic: 8 \n",
      "Words: 0.023*\"http\" + 0.021*\"stks\" + 0.019*\"https\" + 0.013*\"dividend\" + 0.012*\"aapl\" + 0.010*\"stock\" + 0.008*\"target\" + 0.008*\"pop\" + 0.008*\"fb\" + 0.006*\"daily\"\n",
      "\n",
      "\n",
      "Pos Topic: 9 \n",
      "Words: 0.034*\"long\" + 0.025*\"https\" + 0.017*\"look\" + 0.014*\"tsla\" + 0.011*\"like\" + 0.009*\"spy\" + 0.009*\"short\" + 0.007*\"fb\" + 0.007*\"stock\" + 0.007*\"go\"\n",
      "\n",
      "\n",
      "Neg Topic: 0 \n",
      "Words: 0.036*\"stks\" + 0.036*\"http\" + 0.016*\"price\" + 0.016*\"short\" + 0.011*\"support\" + 0.011*\"bear\" + 0.011*\"year\" + 0.011*\"retest\" + 0.011*\"key\" + 0.011*\"weakest\"\n",
      "\n",
      "\n",
      "Neg Topic: 1 \n",
      "Words: 0.059*\"https\" + 0.057*\"tsla\" + 0.053*\"recall\" + 0.043*\"model\" + 0.036*\"tesla\" + 0.018*\"seat\" + 0.017*\"suvs\" + 0.013*\"aapl\" + 0.013*\"fb\" + 0.011*\"motor\"\n",
      "\n",
      "\n",
      "Neg Topic: 2 \n",
      "Words: 0.027*\"stks\" + 0.027*\"http\" + 0.020*\"short\" + 0.013*\"tsla\" + 0.012*\"stock\" + 0.011*\"https\" + 0.008*\"day\" + 0.008*\"low\" + 0.008*\"oil\" + 0.008*\"ddd\"\n",
      "\n",
      "\n",
      "Neg Topic: 3 \n",
      "Words: 0.014*\"day\" + 0.014*\"spy\" + 0.014*\"sell\" + 0.014*\"ba\" + 0.009*\"head\" + 0.009*\"overbought\" + 0.009*\"msft\" + 0.009*\"stks\" + 0.009*\"goog\" + 0.009*\"http\"\n",
      "\n",
      "\n",
      "Neg Topic: 4 \n",
      "Words: 0.018*\"short\" + 0.014*\"bearish\" + 0.014*\"break\" + 0.013*\"red\" + 0.009*\"fb\" + 0.009*\"like\" + 0.009*\"weak\" + 0.009*\"wynn\" + 0.009*\"bell\" + 0.009*\"windows\"\n",
      "\n",
      "\n",
      "Neg Topic: 5 \n",
      "Words: 0.021*\"tsla\" + 0.020*\"https\" + 0.016*\"market\" + 0.015*\"share\" + 0.012*\"aapl\" + 0.012*\"stks\" + 0.012*\"http\" + 0.008*\"spy\" + 0.008*\"downside\" + 0.008*\"move\"\n",
      "\n",
      "\n",
      "Neg Topic: 6 \n",
      "Words: 0.024*\"short\" + 0.024*\"look\" + 0.020*\"sell\" + 0.016*\"go\" + 0.016*\"support\" + 0.016*\"spy\" + 0.016*\"like\" + 0.012*\"weak\" + 0.008*\"https\" + 0.008*\"signal\"\n",
      "\n",
      "\n",
      "Neg Topic: 7 \n",
      "Words: 0.022*\"spy\" + 0.018*\"short\" + 0.013*\"aapl\" + 0.013*\"look\" + 0.013*\"gs\" + 0.013*\"sell\" + 0.013*\"close\" + 0.010*\"https\" + 0.009*\"new\" + 0.009*\"today\"\n",
      "\n",
      "\n",
      "Neg Topic: 8 \n",
      "Words: 0.028*\"sbux\" + 0.027*\"short\" + 0.022*\"downgrade\" + 0.015*\"stks\" + 0.015*\"http\" + 0.015*\"target\" + 0.015*\"bank\" + 0.015*\"deutsche\" + 0.012*\"starbucks\" + 0.012*\"rejoice\"\n",
      "\n",
      "\n",
      "Neg Topic: 9 \n",
      "Words: 0.033*\"short\" + 0.029*\"aapl\" + 0.025*\"sell\" + 0.022*\"https\" + 0.015*\"fb\" + 0.015*\"spy\" + 0.015*\"rat\" + 0.011*\"hit\" + 0.011*\"fall\" + 0.011*\"iwm\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_pos_model.print_topics(-1):\n",
    "    print(\"Pos Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "for idx, topic in lda_neg_model.print_topics(-1):\n",
    "    print(\"Neg Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T14:57:11.014543100Z",
     "start_time": "2023-06-16T14:57:09.522280900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$TZOO a close above 28.64 and we are ready to rock and roll\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 8\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(unseen_document)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m#print(f' This document is from newsgroup {newsgroups_test.target_names[newsgroups_test.target[test_document_idx]]}')\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Data preprocessing step for the unseen document - It is the same preprocessing we have performed for the training data\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m bow_vector \u001B[38;5;241m=\u001B[39m \u001B[43mdictionary\u001B[49m\u001B[38;5;241m.\u001B[39mdoc2bow(preprocess(unseen_document))\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, count \u001B[38;5;129;01min\u001B[39;00m bow_vector:\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdictionary[idx]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcount\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "test_document_idx = 10\n",
    "unseen_document = pos_tweets[test_document_idx]\n",
    "print(unseen_document)\n",
    "\n",
    "#print(f' This document is from newsgroup {newsgroups_test.target_names[newsgroups_test.target[test_document_idx]]}')\n",
    "\n",
    "# Data preprocessing step for the unseen document - It is the same preprocessing we have performed for the training data\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for idx, count in bow_vector:\n",
    "    print(f'{dictionary[idx]}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-16T14:57:11.011042300Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_distribution = lda_model[bow_vector]\n",
    "\n",
    "for index, probability in sorted(topic_distribution, key=lambda tup: -1*tup[1]):\n",
    "    print(\"Index: {}\\nProbability: {}\\t Topic: {}\".format(index, probability, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-16T14:57:11.013543Z"
    }
   },
   "outputs": [],
   "source": [
    "# make list of tuples ready for model training\n",
    "\n",
    "train_set = list(zip(list_a, list_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyLDAvis  For Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Named Entity Recognition (max. 19%)  \n",
    "\n",
    "In scientific research, information extraction can help researchers to discover relevant findings from\n",
    "across a wide body of literature. As a first step, your task is to build a tool for named entity\n",
    "recognition in scientific journal article abstracts. We will be working with the BioNLP 2004 dataset of\n",
    "abstracts from MEDLINE, a database containing journal articles from fields including medicine and\n",
    "pharmacy. The data was collected by searching for the terms ‘human’, ‘blood cells’ and\n",
    "‘transcription factors’, and then annotated with five entity types: DNA, protein, cell type, cell line,\n",
    "RNA. \n",
    "\n",
    "More information can be found in the paper: https://aclanthology.org/W04-1213.pdf .\n",
    "We provide a cache of the data and code for loading the data in ‘data_loader_demo’ in our Github\n",
    "repository, https://github.com/uob-TextAnalytics/intro-labs-public. This script downloaded the data\n",
    "from HuggingFace, where you can also find more information about the dataset:\n",
    "https://huggingface.co/datasets/tner/bionlp2004 .\n",
    "\n",
    "\n",
    "The data is presented in this paper:\n",
    "Nigel Collier, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, and Jin-Dong Kim. 2004. Introduction\n",
    "to the Bio-entity Recognition Task at JNLPBA. In Proceedings of the International Joint Workshop on\n",
    "Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP), pages 73–78,\n",
    "Geneva, Switzerland. COLING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
