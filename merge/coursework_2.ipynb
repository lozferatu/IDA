{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Coursework IDA\n",
    "\n",
    "## Task 1\n",
    "\n",
    "## 1.1. \n",
    "Implement and train a method for automatically classifying texts in the FiQA sentiment analysis\n",
    "dataset as positive, neutral or negative. Refer to the labs, lecture materials and textbook to identify\n",
    "a suitable method. In your report:\n",
    "• Briefly explain how your chosen method works and its main strengths and limitations;\n",
    "• Describe the preprocessing steps and the features you use to represent each text instance;\n",
    "• Explain why you chose those features and preprocessing steps and hypothesise how they\n",
    "will affect your results;\n",
    "• Briefly describe your software implementation.\n",
    "(10 marks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:21.187102900Z",
     "start_time": "2023-05-24T00:30:19.959657100Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use HuggingFace's datasets library to access the financial_phrasebank dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:23.675469100Z",
     "start_time": "2023-05-24T00:30:23.522954800Z"
    }
   },
   "outputs": [],
   "source": [
    "train_files = [\n",
    "    # 'data_cache/FiQA_ABSA_task1/task1_headline_ABSA_train.json',\n",
    "    'data_cache/FiQA_ABSA_task1/task1_post_ABSA_train.json'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:34.981516400Z",
     "start_time": "2023-05-24T00:30:34.816489400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 675\n",
      "Number of labels: 675\n",
      "Number of negative labels: 203\n",
      "Number of neutral labels: 74\n",
      "Number of positive labels: 398\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_fiqa_sa_from_json(json_files):\n",
    "    train_text = []\n",
    "    train_labels = []\n",
    "\n",
    "    # iterate through each tweet file\n",
    "    for file in json_files:\n",
    "        # open file in read mode, with method closes file after getting data stream\n",
    "        with open(file, 'r', encoding = 'utf8') as handle:\n",
    "            # load file object and convert into json object\n",
    "            dataf = json.load(handle)\n",
    "        \n",
    "        \n",
    "        dataf_text = [dataf[k][\"sentence\"] for k in dataf.keys()]\n",
    "        # print(len(dataf_text))\n",
    "        train_text.extend(dataf_text)\n",
    "\n",
    "        dataf_labels = [float(dataf[k][\"info\"][0][\"sentiment_score\"]) for k in dataf.keys()]\n",
    "        # print(len(dataf_labels))\n",
    "        train_labels.extend(dataf_labels)\n",
    "\n",
    "    train_text = np.array(train_text)\n",
    "    train_labels = np.array(train_labels)\n",
    "    \n",
    "    return train_text, train_labels\n",
    "\n",
    "\n",
    "def threshold_scores(scores):\n",
    "    \"\"\"\n",
    "    Convert sentiment scores to discrete labels.\n",
    "    0 = negative.\n",
    "    1 = neutral.\n",
    "    2 = positive.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for score in scores:\n",
    "        if score < -0.2:\n",
    "            labels.append(0)\n",
    "        elif score > 0.2:\n",
    "            labels.append(2)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "            \n",
    "    return np.array(labels)\n",
    "\n",
    "\n",
    "all_text, all_labels = load_fiqa_sa_from_json(train_files)\n",
    "    \n",
    "print(f'Number of instances: {len(all_text)}')\n",
    "print(f'Number of labels: {len(all_labels)}')\n",
    "\n",
    "all_labels = threshold_scores(all_labels)\n",
    "print(f'Number of negative labels: {np.sum(all_labels==0)}')\n",
    "print(f'Number of neutral labels: {np.sum(all_labels==1)}')\n",
    "print(f'Number of positive labels: {np.sum(all_labels==2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:39.921969100Z",
     "start_time": "2023-05-24T00:30:39.767944100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tuple"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(load_fiqa_sa_from_json(train_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:44.228481400Z",
     "start_time": "2023-05-24T00:30:44.095452400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675\n"
     ]
    }
   ],
   "source": [
    "print(len(load_fiqa_sa_from_json(train_files)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:45.923359200Z",
     "start_time": "2023-05-24T00:30:44.757101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances = 459\n",
      "Number of validation instances = 81\n",
      "Number of test instances = 135\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split test data from training data\n",
    "train_documents, test_documents, train_labels, test_labels = train_test_split(\n",
    "    all_text, \n",
    "    all_labels, \n",
    "    test_size=0.2, \n",
    "    stratify=all_labels  # make sure the same proportion of labels is in the test set and training set\n",
    ")\n",
    "\n",
    "# Split validation data from training data\n",
    "train_documents, val_documents, train_labels, val_labels = train_test_split(\n",
    "    train_documents, \n",
    "    train_labels, \n",
    "    test_size=0.15, \n",
    "    stratify=train_labels  # make sure the same proportion of labels is in the test set and training set\n",
    ")\n",
    "\n",
    "print(f'Number of training instances = {len(train_documents)}')\n",
    "print(f'Number of validation instances = {len(val_documents)}')\n",
    "print(f'Number of test instances = {len(test_documents)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:46.149409800Z",
     "start_time": "2023-05-24T00:30:45.925359700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does one instance look like from the training set? \n",
      "\n",
      "$ARIA This is the reason why shorts don't show up...long steady small climbs. The best way to accrue value. IMO\n",
      "...and here is its corresponding label \n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(f'What does one instance look like from the training set? \\n\\n{train_documents[233]}')\n",
    "print(f'...and here is its corresponding label \\n\\n{train_labels[233]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:48.269882200Z",
     "start_time": "2023-05-24T00:30:46.154411200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loz\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mpunkt\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt/english.pickle\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\loz/nltk_data'\n    - 'C:\\\\Users\\\\loz\\\\anaconda3\\\\envs\\\\data_analytics\\\\nltk_data'\n    - 'C:\\\\Users\\\\loz\\\\anaconda3\\\\envs\\\\data_analytics\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\loz\\\\anaconda3\\\\envs\\\\data_analytics\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\loz\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 12\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m word_tokenize(tweets)\n\u001B[0;32m     10\u001B[0m vectorizer \u001B[38;5;241m=\u001B[39m CountVectorizer(tokenizer\u001B[38;5;241m=\u001B[39mTokenizer())  \u001B[38;5;66;03m# construct the vectorizer\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m \u001B[43mvectorizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_documents\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Learn the vocabulary\u001B[39;00m\n\u001B[0;32m     13\u001B[0m X_train \u001B[38;5;241m=\u001B[39m vectorizer\u001B[38;5;241m.\u001B[39mtransform(train_documents)  \u001B[38;5;66;03m# extract training set bags of words\u001B[39;00m\n\u001B[0;32m     14\u001B[0m X_val \u001B[38;5;241m=\u001B[39m vectorizer\u001B[38;5;241m.\u001B[39mtransform(val_documents)  \u001B[38;5;66;03m# extract test set bags of words\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1283\u001B[0m, in \u001B[0;36mCountVectorizer.fit\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   1267\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001B[39;00m\n\u001B[0;32m   1268\u001B[0m \n\u001B[0;32m   1269\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1280\u001B[0m \u001B[38;5;124;03m    Fitted vectorizer.\u001B[39;00m\n\u001B[0;32m   1281\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1282\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_warn_for_unused_params()\n\u001B[1;32m-> 1283\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1284\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001B[0m, in \u001B[0;36mCountVectorizer.fit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   1322\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1323\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpper case characters found in\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1324\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m vocabulary while \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlowercase\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1325\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is True. These entries will not\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1326\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be matched with any documents\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1327\u001B[0m             )\n\u001B[0;32m   1328\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m-> 1330\u001B[0m vocabulary, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_count_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfixed_vocabulary_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1332\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[0;32m   1333\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1201\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[1;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[0;32m   1199\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m raw_documents:\n\u001B[0;32m   1200\u001B[0m     feature_counter \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m-> 1201\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m \u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1202\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1203\u001B[0m             feature_idx \u001B[38;5;241m=\u001B[39m vocabulary[feature]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:115\u001B[0m, in \u001B[0;36m_analyze\u001B[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[0;32m    113\u001B[0m     doc \u001B[38;5;241m=\u001B[39m preprocessor(doc)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 115\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ngrams \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stop_words \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "Cell \u001B[1;32mIn[9], line 8\u001B[0m, in \u001B[0;36mTokenizer.__call__\u001B[1;34m(self, tweets)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, tweets):\n\u001B[1;32m----> 8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mword_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtweets\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001B[0m, in \u001B[0;36mword_tokenize\u001B[1;34m(text, language, preserve_line)\u001B[0m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mword_tokenize\u001B[39m(text, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m, preserve_line\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    115\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;124;03m    Return a tokenized copy of *text*,\u001B[39;00m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;124;03m    using NLTK's recommended word tokenizer\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;124;03m    :type preserve_line: bool\u001B[39;00m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 129\u001B[0m     sentences \u001B[38;5;241m=\u001B[39m [text] \u001B[38;5;28;01mif\u001B[39;00m preserve_line \u001B[38;5;28;01melse\u001B[39;00m \u001B[43msent_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m    131\u001B[0m         token \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m sentences \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m _treebank_word_tokenizer\u001B[38;5;241m.\u001B[39mtokenize(sent)\n\u001B[0;32m    132\u001B[0m     ]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001B[0m, in \u001B[0;36msent_tokenize\u001B[1;34m(text, language)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msent_tokenize\u001B[39m(text, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     97\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001B[39;00m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;124;03m    :param language: the model name in the Punkt corpus\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 106\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtokenizers/punkt/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mlanguage\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.pickle\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mtokenize(text)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\nltk\\data.py:750\u001B[0m, in \u001B[0;36mload\u001B[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001B[0m\n\u001B[0;32m    747\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<<Loading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresource_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m>>\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    749\u001B[0m \u001B[38;5;66;03m# Load the resource.\u001B[39;00m\n\u001B[1;32m--> 750\u001B[0m opened_resource \u001B[38;5;241m=\u001B[39m \u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresource_url\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    752\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    753\u001B[0m     resource_val \u001B[38;5;241m=\u001B[39m opened_resource\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\nltk\\data.py:876\u001B[0m, in \u001B[0;36m_open\u001B[1;34m(resource_url)\u001B[0m\n\u001B[0;32m    873\u001B[0m protocol, path_ \u001B[38;5;241m=\u001B[39m split_resource_url(resource_url)\n\u001B[0;32m    875\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m protocol \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m protocol\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnltk\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 876\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mopen()\n\u001B[0;32m    877\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m protocol\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    878\u001B[0m     \u001B[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001B[39;00m\n\u001B[0;32m    879\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m find(path_, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mopen()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_analytics\\lib\\site-packages\\nltk\\data.py:583\u001B[0m, in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    581\u001B[0m sep \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m70\u001B[39m\n\u001B[0;32m    582\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 583\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mpunkt\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt/english.pickle\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\loz/nltk_data'\n    - 'C:\\\\Users\\\\loz\\\\anaconda3\\\\envs\\\\data_analytics\\\\nltk_data'\n    - 'C:\\\\Users\\\\loz\\\\anaconda3\\\\envs\\\\data_analytics\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\loz\\\\anaconda3\\\\envs\\\\data_analytics\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\loz\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# CountVectorizer can do its own tokenization, but for consistency we want to\n",
    "# carry on using WordNetTokenizer. We write a small wrapper class to enable this:\n",
    "class Tokenizer(object):\n",
    "    def __call__(self, tweets):\n",
    "        return word_tokenize(tweets)\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=Tokenizer())  # construct the vectorizer\n",
    "\n",
    "vectorizer.fit(train_documents)  # Learn the vocabulary\n",
    "X_train = vectorizer.transform(train_documents)  # extract training set bags of words\n",
    "X_val = vectorizer.transform(val_documents)  # extract test set bags of words\n",
    "X_test = vectorizer.transform(test_documents)  # extract test set bags of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = classifier.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "acc = accuracy_score(val_labels, y_val_pred)\n",
    "print(f'Accuracy = {acc}')\n",
    "\n",
    "prec = precision_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Precision (macro average) = {prec}')\n",
    "\n",
    "rec = recall_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Recall (macro average) = {rec}')\n",
    "\n",
    "f1 = f1_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'F1 score (macro average) = {f1}')\n",
    "\n",
    "# We can get all of these with a per-class breakdown using classification_report:\n",
    "print(classification_report(val_labels, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:48.969042900Z",
     "start_time": "2023-05-24T00:30:48.709984500Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'vocabulary_'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m vocabulary \u001B[38;5;241m=\u001B[39m \u001B[43mvectorizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocabulary_\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m### CHANGE THE NAME OF THE CLASSIFIER VARIABLE BELOW TO USE YOUR TRAINED CLASSIFIER\u001B[39;00m\n\u001B[0;32m      6\u001B[0m feat_likelihoods \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexp(classifier\u001B[38;5;241m.\u001B[39mfeature_log_prob_)  \u001B[38;5;66;03m# Use exponential to convert the logs back to probabilities\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'CountVectorizer' object has no attribute 'vocabulary_'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "### CHANGE THE NAME OF THE CLASSIFIER VARIABLE BELOW TO USE YOUR TRAINED CLASSIFIER\n",
    "feat_likelihoods = np.exp(classifier.feature_log_prob_)  # Use exponential to convert the logs back to probabilities\n",
    "###\n",
    "\n",
    "# WRITE YOUR CODE HERE\n",
    "print(feat_likelihoods[:, vocabulary['a']])\n",
    "print(feat_likelihoods[:, vocabulary['it']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logistic Regression Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:49.730063500Z",
     "start_time": "2023-05-24T00:30:49.471006100Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinear_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LogisticRegression\n\u001B[0;32m      3\u001B[0m classifier \u001B[38;5;241m=\u001B[39m LogisticRegression()\n\u001B[1;32m----> 4\u001B[0m classifier\u001B[38;5;241m.\u001B[39mfit(\u001B[43mX_train\u001B[49m, train_labels)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:50.083401400Z",
     "start_time": "2023-05-24T00:30:49.841089Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m y_val_pred \u001B[38;5;241m=\u001B[39m classifier\u001B[38;5;241m.\u001B[39mpredict(\u001B[43mX_val\u001B[49m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X_val' is not defined"
     ]
    }
   ],
   "source": [
    "y_val_pred = classifier.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Evaluate Method\n",
    "\n",
    "Evaluate your method, then interpret and discuss your results. Include the following points:\n",
    "• Define your performance metrics and state their limitations;\n",
    "• Describe the testing procedure (e.g., how you used each split of the dataset);\n",
    "• Show your results using suitable plots or tables;\n",
    "• How could you improve the method or experimental process? Consider the errors that your\n",
    "method makes.  \n",
    "(9 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:50.470485800Z",
     "start_time": "2023-05-24T00:30:50.464484500Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:50.919140800Z",
     "start_time": "2023-05-24T00:30:50.663528800Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_val_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# WRITE YOUR CODE HERE\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m accuracy_score, precision_score, recall_score, f1_score, classification_report\n\u001B[1;32m----> 4\u001B[0m acc \u001B[38;5;241m=\u001B[39m accuracy_score(val_labels, \u001B[43my_val_pred\u001B[49m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAccuracy = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00macc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      7\u001B[0m prec \u001B[38;5;241m=\u001B[39m precision_score(val_labels, y_val_pred, average\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmacro\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'y_val_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "acc = accuracy_score(val_labels, y_val_pred)\n",
    "print(f'Accuracy = {acc}')\n",
    "\n",
    "prec = precision_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Precision (macro average) = {prec}')\n",
    "\n",
    "rec = recall_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'Recall (macro average) = {rec}')\n",
    "\n",
    "f1 = f1_score(val_labels, y_val_pred, average='macro')\n",
    "print(f'F1 score (macro average) = {f1}')\n",
    "\n",
    "# We can get all of these with a per-class breakdown using classification_report:\n",
    "print(classification_report(val_labels, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:51.226655Z",
     "start_time": "2023-05-24T00:30:51.215652800Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:51.439231400Z",
     "start_time": "2023-05-24T00:30:51.420227100Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:51.684626700Z",
     "start_time": "2023-05-24T00:30:51.671623500Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Common Themes & Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. Can you identify common themes or topics associated with negative sentiment or positive\n",
    "sentiment in this dataset?\n",
    "• Explain the method you use to identify themes or topics;\n",
    "• Show your results (e.g., by listing or visualising example topics or themes);\n",
    "• Interpret the results and summarise the limitations of your approach.\n",
    "(12 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-05-24T00:30:53.006019Z",
     "start_time": "2023-05-24T00:30:52.770966200Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'vocabulary_'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m n_feats_to_show \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Flip the index so that values are keys and keys are values:\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m keys \u001B[38;5;241m=\u001B[39m \u001B[43mvectorizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocabulary_\u001B[49m\u001B[38;5;241m.\u001B[39mvalues()\n\u001B[0;32m      5\u001B[0m values \u001B[38;5;241m=\u001B[39m vectorizer\u001B[38;5;241m.\u001B[39mvocabulary_\u001B[38;5;241m.\u001B[39mkeys()\n\u001B[0;32m      6\u001B[0m vocab_inverted \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(keys, values))\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'CountVectorizer' object has no attribute 'vocabulary_'"
     ]
    }
   ],
   "source": [
    "n_feats_to_show = 10\n",
    "\n",
    "# Flip the index so that values are keys and keys are values:\n",
    "keys = vectorizer.vocabulary_.values()\n",
    "values = vectorizer.vocabulary_.keys()\n",
    "vocab_inverted = dict(zip(keys, values))\n",
    "\n",
    "for c, weights_c in enumerate(classifier.coef_):\n",
    "    print(f'\\nWeights for class {c}:\\n')\n",
    "    strongest_idxs = np.argsort(weights_c)[-n_feats_to_show:]\n",
    "\n",
    "    for idx in strongest_idxs:\n",
    "        print(f'{vocab_inverted[idx]} with weight {weights_c[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_index = all_labels == 2  # compare predictions to gold labels\n",
    "neg_index = all_labels == 0  # compare predictions to gold labels\n",
    "# get the text of tweets where the classifier made an error:\n",
    "pos_tweets = np.array(all_text)[pos_index]\n",
    "neg_tweets = np.array(all_text)[neg_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slowly adding some $FIO here but gotta be careful. This will be one of biggest winners in 2012\n",
      "I am not optimistic about $amzn both fundementals and charts look like poopoo this quarter.\n"
     ]
    }
   ],
   "source": [
    "#type(pos_tweets)\n",
    "print(pos_tweets[0])\n",
    "print(neg_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_pos = []\n",
    "processed_neg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS # find stopwords\n",
    "\n",
    "np.random.seed(400)  # We fix the random seed to ensure we get consistent results when we repeat the lab.\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in simple_preprocess(text) :  # Tokenize, remove very short and very long words, convert to lower case, remove words containing non-letter characters\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(WordNetLemmatizer().lemmatize(token, 'v'))\n",
    "            \n",
    "    return result\n",
    "\n",
    "# Create lists of preprocessed documents\n",
    "for tweet in pos_tweets:\n",
    "    processed_pos.append(preprocess(tweet))\n",
    "    \n",
    "for tweet in neg_tweets:\n",
    "    processed_neg.append(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slowly', 'add', 'fio', 'gotta', 'careful', 'biggest', 'winners']\n",
      "['optimistic', 'amzn', 'fundementals', 'chart', 'look', 'like', 'poopoo', 'quarter']\n"
     ]
    }
   ],
   "source": [
    "print(processed_pos[0])\n",
    "print(processed_neg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1514 unique tokens: ['add', 'biggest', 'careful', 'fio', 'gotta']...)\n",
      "Dictionary(887 unique tokens: ['amzn', 'chart', 'fundementals', 'like', 'look']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary_pos = Dictionary(processed_pos) # construct word<->id mappings - it does it in alphabetical order\n",
    "print(dictionary_pos)\n",
    "\n",
    "pos_bow_corpus = [dictionary_pos.doc2bow(tweet) for tweet in processed_pos]\n",
    "\n",
    "dictionary_neg = Dictionary(processed_neg) # construct word<->id mappings - it does it in alphabetical order\n",
    "print(dictionary_neg)\n",
    "\n",
    "neg_bow_corpus = [dictionary_neg.doc2bow(tweet) for tweet in processed_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "796"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "lda_pos_model =  LdaModel(pos_bow_corpus, \n",
    "                      num_topics=10, \n",
    "                      id2word=dictionary_pos,                                    \n",
    "                      passes=10,\n",
    "                    ) \n",
    "\n",
    "lda_neg_model =  LdaModel(neg_bow_corpus, \n",
    "                      num_topics=10, \n",
    "                      id2word=dictionary_neg,                                    \n",
    "                      passes=10,\n",
    "                    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos Topic: 0 \n",
      "Words: 0.059*\"https\" + 0.016*\"buy\" + 0.014*\"upgrade\" + 0.012*\"googl\" + 0.011*\"fb\" + 0.009*\"tsla\" + 0.009*\"price\" + 0.009*\"sell\" + 0.009*\"trade\" + 0.009*\"time\"\n",
      "\n",
      "\n",
      "Pos Topic: 1 \n",
      "Words: 0.030*\"http\" + 0.024*\"stks\" + 0.023*\"buy\" + 0.021*\"bullish\" + 0.020*\"aapl\" + 0.019*\"breakout\" + 0.018*\"stock\" + 0.012*\"high\" + 0.012*\"double\" + 0.010*\"long\"\n",
      "\n",
      "\n",
      "Pos Topic: 2 \n",
      "Words: 0.024*\"good\" + 0.021*\"long\" + 0.016*\"close\" + 0.016*\"short\" + 0.015*\"buy\" + 0.014*\"aapl\" + 0.014*\"dividend\" + 0.011*\"hold\" + 0.010*\"https\" + 0.010*\"look\"\n",
      "\n",
      "\n",
      "Pos Topic: 3 \n",
      "Words: 0.023*\"aapl\" + 0.023*\"http\" + 0.023*\"stks\" + 0.020*\"today\" + 0.017*\"look\" + 0.017*\"strong\" + 0.013*\"bounce\" + 0.012*\"nice\" + 0.011*\"buy\" + 0.011*\"break\"\n",
      "\n",
      "\n",
      "Pos Topic: 4 \n",
      "Words: 0.043*\"long\" + 0.017*\"aapl\" + 0.015*\"hod\" + 0.013*\"pop\" + 0.013*\"tsla\" + 0.013*\"small\" + 0.012*\"buy\" + 0.011*\"stks\" + 0.011*\"http\" + 0.010*\"https\"\n",
      "\n",
      "\n",
      "Pos Topic: 5 \n",
      "Words: 0.039*\"http\" + 0.038*\"stks\" + 0.013*\"https\" + 0.013*\"today\" + 0.013*\"new\" + 0.012*\"stock\" + 0.011*\"higher\" + 0.009*\"chart\" + 0.009*\"come\" + 0.009*\"volume\"\n",
      "\n",
      "\n",
      "Pos Topic: 6 \n",
      "Words: 0.032*\"http\" + 0.030*\"stks\" + 0.023*\"call\" + 0.018*\"tsla\" + 0.016*\"buy\" + 0.014*\"signal\" + 0.013*\"https\" + 0.012*\"long\" + 0.010*\"pt\" + 0.009*\"fb\"\n",
      "\n",
      "\n",
      "Pos Topic: 7 \n",
      "Words: 0.035*\"stks\" + 0.035*\"http\" + 0.022*\"go\" + 0.013*\"bounce\" + 0.011*\"long\" + 0.011*\"green\" + 0.011*\"apple\" + 0.010*\"company\" + 0.009*\"aapl\" + 0.008*\"time\"\n",
      "\n",
      "\n",
      "Pos Topic: 8 \n",
      "Words: 0.030*\"https\" + 0.017*\"stks\" + 0.017*\"http\" + 0.015*\"long\" + 0.014*\"ma\" + 0.011*\"increase\" + 0.010*\"look\" + 0.009*\"good\" + 0.009*\"base\" + 0.009*\"strategy\"\n",
      "\n",
      "\n",
      "Pos Topic: 9 \n",
      "Words: 0.022*\"like\" + 0.021*\"look\" + 0.020*\"long\" + 0.014*\"day\" + 0.014*\"https\" + 0.012*\"amzn\" + 0.012*\"run\" + 0.012*\"low\" + 0.011*\"stks\" + 0.011*\"http\"\n",
      "\n",
      "\n",
      "Neg Topic: 0 \n",
      "Words: 0.033*\"short\" + 0.031*\"stks\" + 0.031*\"http\" + 0.020*\"aapl\" + 0.020*\"look\" + 0.017*\"like\" + 0.011*\"fb\" + 0.011*\"lower\" + 0.011*\"sell\" + 0.009*\"rejoice\"\n",
      "\n",
      "\n",
      "Neg Topic: 1 \n",
      "Words: 0.020*\"short\" + 0.017*\"market\" + 0.014*\"downgrade\" + 0.013*\"aapl\" + 0.011*\"tsla\" + 0.010*\"share\" + 0.010*\"time\" + 0.010*\"day\" + 0.010*\"lower\" + 0.010*\"stks\"\n",
      "\n",
      "\n",
      "Neg Topic: 2 \n",
      "Words: 0.011*\"weak\" + 0.011*\"gs\" + 0.011*\"short\" + 0.011*\"good\" + 0.011*\"look\" + 0.011*\"downside\" + 0.011*\"bay\" + 0.011*\"million\" + 0.006*\"break\" + 0.006*\"target\"\n",
      "\n",
      "\n",
      "Neg Topic: 3 \n",
      "Words: 0.021*\"sell\" + 0.021*\"stks\" + 0.021*\"http\" + 0.016*\"min\" + 0.011*\"aapl\" + 0.011*\"qqq\" + 0.011*\"spy\" + 0.011*\"signal\" + 0.011*\"rt\" + 0.011*\"retest\"\n",
      "\n",
      "\n",
      "Neg Topic: 4 \n",
      "Words: 0.029*\"tsla\" + 0.029*\"https\" + 0.018*\"short\" + 0.018*\"get\" + 0.018*\"sell\" + 0.010*\"model\" + 0.010*\"recall\" + 0.009*\"new\" + 0.009*\"authentication\" + 0.009*\"jump\"\n",
      "\n",
      "\n",
      "Neg Topic: 5 \n",
      "Words: 0.057*\"https\" + 0.050*\"tsla\" + 0.049*\"recall\" + 0.039*\"model\" + 0.035*\"tesla\" + 0.017*\"seat\" + 0.017*\"suvs\" + 0.013*\"stks\" + 0.013*\"http\" + 0.013*\"fb\"\n",
      "\n",
      "\n",
      "Neg Topic: 6 \n",
      "Words: 0.023*\"http\" + 0.022*\"spy\" + 0.019*\"stks\" + 0.015*\"buy\" + 0.011*\"bearish\" + 0.011*\"chart\" + 0.011*\"support\" + 0.008*\"fb\" + 0.008*\"gap\" + 0.008*\"day\"\n",
      "\n",
      "\n",
      "Neg Topic: 7 \n",
      "Words: 0.041*\"https\" + 0.029*\"sbux\" + 0.020*\"deutsche\" + 0.020*\"bank\" + 0.020*\"downgrade\" + 0.018*\"starbucks\" + 0.016*\"short\" + 0.011*\"miss\" + 0.011*\"ntap\" + 0.011*\"stks\"\n",
      "\n",
      "\n",
      "Neg Topic: 8 \n",
      "Words: 0.024*\"short\" + 0.017*\"https\" + 0.012*\"aapl\" + 0.012*\"fall\" + 0.012*\"today\" + 0.008*\"sell\" + 0.008*\"spy\" + 0.008*\"go\" + 0.008*\"low\" + 0.008*\"hit\"\n",
      "\n",
      "\n",
      "Neg Topic: 9 \n",
      "Words: 0.020*\"short\" + 0.020*\"spy\" + 0.020*\"close\" + 0.010*\"week\" + 0.010*\"continue\" + 0.010*\"think\" + 0.010*\"gain\" + 0.010*\"market\" + 0.010*\"ma\" + 0.010*\"resistance\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_pos_model.print_topics(-1):\n",
    "    print(\"Pos Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "for idx, topic in lda_neg_model.print_topics(-1):\n",
    "    print(\"Neg Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$TZOO a close above 28.64 and we are ready to rock and roll\n",
      "close: 1\n",
      "ready: 1\n",
      "rock: 1\n",
      "roll: 1\n",
      "tzoo: 1\n"
     ]
    }
   ],
   "source": [
    "test_document_idx = 10\n",
    "unseen_document = pos_tweets[test_document_idx]\n",
    "print(unseen_document)\n",
    "\n",
    "#print(f' This document is from newsgroup {newsgroups_test.target_names[newsgroups_test.target[test_document_idx]]}')\n",
    "\n",
    "# Data preprocessing step for the unseen document - It is the same preprocessing we have performed for the training data\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for idx, count in bow_vector:\n",
    "    print(f'{dictionary[idx]}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 14\n",
      "Probability: 0.4022486209869385\t Topic: 0.016*\"low\" + 0.015*\"breakout\" + 0.011*\"long\" + 0.011*\"supply\" + 0.011*\"master\"\n",
      "Index: 6\n",
      "Probability: 0.2308562695980072\t Topic: 0.038*\"buy\" + 0.023*\"https\" + 0.019*\"stks\" + 0.019*\"http\" + 0.019*\"long\"\n",
      "Index: 17\n",
      "Probability: 0.22521528601646423\t Topic: 0.035*\"https\" + 0.023*\"time\" + 0.018*\"dip\" + 0.018*\"upside\" + 0.018*\"move\"\n"
     ]
    }
   ],
   "source": [
    "topic_distribution = lda_model[bow_vector]\n",
    "\n",
    "for index, probability in sorted(topic_distribution, key=lambda tup: -1*tup[1]):\n",
    "    print(\"Index: {}\\nProbability: {}\\t Topic: {}\".format(index, probability, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of tuples ready for model training\n",
    "\n",
    "train_set = list(zip(list_a, list_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyLDAvis  For Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Named Entity Recognition (max. 19%)  \n",
    "\n",
    "In scientific research, information extraction can help researchers to discover relevant findings from\n",
    "across a wide body of literature. As a first step, your task is to build a tool for named entity\n",
    "recognition in scientific journal article abstracts. We will be working with the BioNLP 2004 dataset of\n",
    "abstracts from MEDLINE, a database containing journal articles from fields including medicine and\n",
    "pharmacy. The data was collected by searching for the terms ‘human’, ‘blood cells’ and\n",
    "‘transcription factors’, and then annotated with five entity types: DNA, protein, cell type, cell line,\n",
    "RNA. \n",
    "\n",
    "More information can be found in the paper: https://aclanthology.org/W04-1213.pdf .\n",
    "We provide a cache of the data and code for loading the data in ‘data_loader_demo’ in our Github\n",
    "repository, https://github.com/uob-TextAnalytics/intro-labs-public. This script downloaded the data\n",
    "from HuggingFace, where you can also find more information about the dataset:\n",
    "https://huggingface.co/datasets/tner/bionlp2004 .\n",
    "\n",
    "\n",
    "The data is presented in this paper:\n",
    "Nigel Collier, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, and Jin-Dong Kim. 2004. Introduction\n",
    "to the Bio-entity Recognition Task at JNLPBA. In Proceedings of the International Joint Workshop on\n",
    "Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP), pages 73–78,\n",
    "Geneva, Switzerland. COLING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
