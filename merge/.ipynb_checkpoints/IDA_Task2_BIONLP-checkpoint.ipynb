{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc4e9a-df49-45e0-98e6-ebd49cf54044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ec39563-772c-4acb-a00c-4a1dd573162e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BIONLP 2004\n",
    "\n",
    "This dataset contains abstracts from MEDLINE, a database containing journal articles from fields including medicine and pharmacy. \n",
    "The data was collected by searching for the terms ‘human’, ‘blood cells’ and ‘transcription factors’, and then annotated with five entity types: DNA, protein, cell type, cell line, RNA. \n",
    "\n",
    "More information in the paper: https://aclanthology.org/W04-1213.pdf\n",
    "\n",
    "The data can be downloaded from HuggingFace: https://huggingface.co/datasets/tner/bionlp2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89a2e6-3882-4021-9e34-dd0f919cc2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e09e77-9177-4360-a684-9531dbcf543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use HuggingFace's datasets library to access the financial_phrasebank dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d353ede4-863a-4378-976a-4f5607cd7a68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset bio_nlp2004 (./data_cache\\tner___bio_nlp2004\\bionlp2004\\1.0.0\\9f41d3f0270b773c2762dee333ae36c29331e2216114a57081f77639fdb5e904)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b761f6535b794c7fb02c01043e85e942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is a dictionary with 3 splits: \n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 16619\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 1927\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 3856\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"tner/bionlp2004\", \n",
    "    cache_dir='./data_cache'\n",
    ")\n",
    "\n",
    "print(f'The dataset is a dictionary with {len(dataset)} splits: \\n\\n{dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b64ae-f9f9-454f-9849-ef77d684dd3e",
   "metadata": {},
   "source": [
    "The dataset is already split into train, validation and test. It may be useful to reformat the DatasetDict object into lists of sentences and tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3e7c8c63-c92e-48c8-8563-44e9f472e158",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DatasetDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[220], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#dataset['train'][1]\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_set \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetDict\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DatasetDict' is not defined"
     ]
    }
   ],
   "source": [
    "#dataset['train'][1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "423b1fe9-4c8b-47c2-b79b-f460e8cc7112",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_ner = [item['tokens'] for item in dataset['train']]\n",
    "train_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['train']]\n",
    "\n",
    "val_sentences_ner = [item['tokens'] for item in dataset['validation']]\n",
    "val_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['validation']]\n",
    "\n",
    "test_sentences_ner = [item['tokens'] for item in dataset['test']]\n",
    "test_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da7896a-2dc0-4985-8627-48c77834f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences = 16619\n",
      "Number of validation sentences = 1927\n",
      "Number of test sentences = 3856\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training sentences = {len(train_sentences_ner)}')\n",
    "print(f'Number of validation sentences = {len(val_sentences_ner)}')\n",
    "print(f'Number of test sentences = {len(test_sentences_ner)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d000688-9fbb-4ea3-a95c-becf496ed43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does one instance look like from the training set? \n",
      "\n",
      "['Hence', ',', 'PPAR', 'can', 'positively', 'or', 'negatively', 'influence', 'TH', 'action', 'depending', 'on', 'TRE', 'structure', 'and', 'THR', 'isotype', '.']\n",
      "...and here is its corresponding label \n",
      "\n",
      "['0', '0', '3', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '3', '4', '0']\n"
     ]
    }
   ],
   "source": [
    "print(f'What does one instance look like from the training set? \\n\\n{train_sentences_ner[234]}')\n",
    "print(f'...and here is its corresponding label \\n\\n{train_labels_ner[234]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c293ad5-29d4-4f5f-9213-a37ed2853830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: ['0' '1' '10' '2' '3' '4' '5' '6' '7' '8' '9']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of unique labels: {np.unique(np.concatenate(train_labels_ner))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3fc291-2394-49f4-99b4-9ba6e32680fe",
   "metadata": {},
   "source": [
    "These are the tags used to annotate the entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "251f5652-afe6-4030-94e6-bf91379b34e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-DNA', 2: 'I-DNA', 3: 'B-protein', 4: 'I-protein', 5: 'B-cell_type', 6: 'I-cell_type', 7: 'B-cell_line', 8: 'I-cell_line', 9: 'B-RNA', 10: 'I-RNA'}\n"
     ]
    }
   ],
   "source": [
    "# mapping from labels to the tags\n",
    "\n",
    "id2label = {\n",
    "    \"O\": 0,\n",
    "    \"B-DNA\": 1,\n",
    "    \"I-DNA\": 2,\n",
    "    \"B-protein\": 3,\n",
    "    \"I-protein\": 4,\n",
    "    \"B-cell_type\": 5,\n",
    "    \"I-cell_type\": 6,\n",
    "    \"B-cell_line\": 7,\n",
    "    \"I-cell_line\": 8,\n",
    "    \"B-RNA\": 9,\n",
    "    \"I-RNA\": 10\n",
    "}\n",
    "\n",
    "label2id = {v:k for k, v in id2label.items()}\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3a6d542-7a38-4457-aa18-312f66d9d0f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#train_sentences_ner[0]\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_set \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m], [id2label[tok] \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner_tags\u001b[39m\u001b[38;5;124m'\u001b[39m]])) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_dataset\u001b[49m][:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#train_sentences_ner[0]\n",
    "train_set = [list(zip(s['tokens'], [id2label[tok] for tok in s['ner_tags']])) for s in train_dataset][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e60b82e0-c93d-4bf3-af10-5db0e0423c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label ref from string to label \n",
    "int_train_labels = []\n",
    "for sent in train_labels_ner:\n",
    "    sent = list(map(int, sent)) \n",
    "    int_train_labels.append(sent)\n",
    "    #print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0354cac2-1496-4571-bb6e-91518009f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# working int to label\n",
    "train_lab_full = []\n",
    "\n",
    "for vector in int_train_labels:\n",
    "    #vector = list(map(int, vector)) \n",
    "    #print(sent)\n",
    "    sent_labels = []\n",
    "    for label in vector:\n",
    "        converted  = label2id[label]\n",
    "        sent_labels.append(converted)\n",
    "        #print(converted)\n",
    "        #res = list(zip(train_sentences_ner, train_labels_ner[idx]))\n",
    "        #train_set.append(res)\n",
    "    train_lab_full.append(list(sent_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3e7aa3b6-fdb7-4217-a8d2-3ac82de3a56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "['O', 'B-protein', 'O', 'O', 'O', 'O', 'O', 'O', 'B-protein', 'I-protein', 'O', 'O', 'O', 'O', 'B-protein', 'I-protein', 'O', 'O', 'O', 'O', 'B-protein', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "36\n",
      "['0', '3', '0', '0', '0', '0', '0', '0', '3', '4', '0', '0', '0', '0', '3', '4', '0', '0', '0', '0', '3', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "print(len(train_lab_full[1]))\n",
    "print(train_lab_full[1])\n",
    "print(len(train_labels_ner[1]))\n",
    "print(train_labels_ner[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ef61c99b-01c8-4b54-aaa0-d307edc75c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working zip\n",
    "train_set = [list(zip(sentence, train_lab_full[idx])) for idx, sentence in enumerate(train_sentences_ner)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "91297b35-13b9-472b-9a76-0dcd03aaf259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "6e007294-cab0-4117-990f-0d3dea1b33fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label ref from string to label \n",
    "int_test_labels = []\n",
    "for sent in test_labels_ner:\n",
    "    sent = list(map(int, sent)) \n",
    "    int_test_labels.append(sent)\n",
    "    #print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a5dc1531-c45e-4fdb-9bd7-dd83aa509d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working int to label\n",
    "test_lab_full = []\n",
    "\n",
    "for vector in int_test_labels:\n",
    "    #vector = list(map(int, vector)) \n",
    "    #print(sent)\n",
    "    sent_labels = []\n",
    "    for label in vector:\n",
    "        converted  = label2id[label]\n",
    "        sent_labels.append(converted)\n",
    "        #print(converted)\n",
    "        #res = list(zip(train_sentences_ner, train_labels_ner[idx]))\n",
    "        #train_set.append(res)\n",
    "    test_lab_full.append(list(sent_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a664ee0e-c6ad-472a-8d5d-72765fb9b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [list(zip(sentence, test_lab_full[idx])) for idx, sentence in enumerate(test_sentences_ner)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "17a3b2ef-6e94-4875-9e79-1e3958b2480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c572af-5193-43db-910e-fb801e306c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7316a2ee-b47a-4083-970e-24435dd375c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "93874e80-fde6-4c08-a214-5b7389656d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sentences_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eda321-8c5f-4c52-8929-fe1eee365c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f7215037-4a74-4312-baac-dbea5f114da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Train a CRF NER tagger\n",
    "def train_CRF_NER_tagger(train_set):\n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    tagger = nltk.tag.CRFTagger()\n",
    "    tagger.train(train_set, 'model.crf.tagger')\n",
    "    return tagger  # return the trained model\n",
    "\n",
    "tagger = train_CRF_NER_tagger(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3c0e9ddc-5452-4870-9981-215a3b74bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = tagger.tag_sents(test_sentences_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e551b5c1-1b3a-4133-9bef-d9c7e1ffa4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "269e4ce0-fd92-410a-8722-c04a1a5bb5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for class protein = 0.038578411943453565\n",
      "F1 score for class cell_type = 0\n",
      "F1 score for class DNA = 0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'cell_line'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[226], line 86\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1 score for class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mne_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMacro-average f1 score = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(f1_per_class)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m \u001b[43mcal_span_level_f1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_tags\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[226], line 55\u001b[0m, in \u001b[0;36mcal_span_level_f1\u001b[1;34m(test_sents, test_sents_with_pred)\u001b[0m\n\u001b[0;32m     52\u001b[0m true_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     53\u001b[0m false_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m span \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpred_spans\u001b[49m\u001b[43m[\u001b[49m\u001b[43mne_type\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m span \u001b[38;5;129;01min\u001b[39;00m gold_spans[ne_type]:\n\u001b[0;32m     57\u001b[0m         true_pos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cell_line'"
     ]
    }
   ],
   "source": [
    "def extract_spans(tagged_sents):\n",
    "    \"\"\"\n",
    "    Extract a list of tagged spans for each named entity type, \n",
    "    where each span is represented by a tuple containing the \n",
    "    start token and end token indexes.\n",
    "    \n",
    "    returns: a dictionary containing a list of spans for each entity type.\n",
    "    \"\"\"\n",
    "    spans = {}\n",
    "        \n",
    "    for sidx, sent in enumerate(tagged_sents):\n",
    "        start = -1\n",
    "        entity_type = None\n",
    "        for i, (tok, lab) in enumerate(sent):\n",
    "            if 'B-' in lab:\n",
    "                start = i\n",
    "                end = i + 1\n",
    "                entity_type = lab[2:]\n",
    "            elif 'I-' in lab:\n",
    "                end = i + 1\n",
    "            elif lab == 'O' and start >= 0:\n",
    "                \n",
    "                if entity_type not in spans:\n",
    "                    spans[entity_type] = []\n",
    "                \n",
    "                spans[entity_type].append((start, end, sidx))\n",
    "                start = -1      \n",
    "        # Sometimes an I-token is the last token in the sentence, so we still have to add the span to the list\n",
    "        if start >= 0:    \n",
    "            if entity_type not in spans:\n",
    "                spans[entity_type] = []\n",
    "                \n",
    "            spans[entity_type].append((start, end, sidx))\n",
    "                \n",
    "    return spans\n",
    "\n",
    "\n",
    "def cal_span_level_f1(test_sents, test_sents_with_pred):\n",
    "    # get a list of spans from the test set labels\n",
    "    gold_spans = extract_spans(test_sents)\n",
    "\n",
    "    # get a list of spans predicted by our tagger\n",
    "    pred_spans = extract_spans(test_sents_with_pred)\n",
    "    \n",
    "    # compute the metrics for each class:\n",
    "    f1_per_class = []\n",
    "    \n",
    "    ne_types = gold_spans.keys()  # get the list of named entity types (not the tags)\n",
    "    \n",
    "    for ne_type in ne_types:\n",
    "        # compute the confusion matrix\n",
    "        true_pos = 0\n",
    "        false_pos = 0\n",
    "        \n",
    "        for span in pred_spans[ne_type]:\n",
    "            if span in gold_spans[ne_type]:\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                false_pos += 1\n",
    "                \n",
    "        false_neg = 0\n",
    "        for span in gold_spans[ne_type]:\n",
    "            if span not in pred_spans[ne_type]:\n",
    "                false_neg += 1\n",
    "                \n",
    "        if true_pos + false_pos == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = true_pos / float(true_pos + false_pos)\n",
    "            \n",
    "        if true_pos + false_neg == 0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = true_pos / float(true_pos + false_neg)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            \n",
    "        f1_per_class.append(f1)\n",
    "        print(f'F1 score for class {ne_type} = {f1}')\n",
    "        \n",
    "    print(f'Macro-average f1 score = {np.mean(f1_per_class)}')\n",
    "\n",
    "cal_span_level_f1(test_set, predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63552679-767f-46d7-9792-37d02f501734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
